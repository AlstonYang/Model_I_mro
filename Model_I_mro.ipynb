{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import related package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import tensorflow package for modeling\n",
    "import tensorflow as tf\n",
    "\n",
    "## Data processing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "## Min-max normalization\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "## Plot the graph\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "## Initializing module\n",
    "from sklearn.linear_model import LinearRegression\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "## Copy module\n",
    "import copy\n",
    "\n",
    "## Used to calculate the training time\n",
    "import time\n",
    "\n",
    "## Set the GUP environment\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Control memory usage space for GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpus = tf.config.list_physical_devices(device_type='GPU')\n",
    "\n",
    "# tf.config.set_logical_device_configuration(\n",
    "#     gpus[0],\n",
    "#     [tf.config.LogicalDeviceConfiguration(memory_limit=2048),\n",
    "#      tf.config.LogicalDeviceConfiguration(memory_limit=2048)]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.client.session.Session at 0x7fda901d8390>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction =0.5\n",
    "tf.compat.v1.Session(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpu_options = tf.compat.v1.GPUOptions(per_process_gpu_memory_fraction=0.5)\n",
    "# sess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(gpu_options=gpu_options))\n",
    "# tf.compat.v1.keras.backend.set_session(sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read(path):\n",
    "    return pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildTrain(train, pastWeek=4, futureWeek=4, defaultWeek=1):\n",
    "    X_train, Y_train = [], []\n",
    "    for i in range(train.shape[0]-futureWeek-pastWeek):\n",
    "        X = np.array(train.iloc[i:i+defaultWeek])\n",
    "        X = np.append(X,train[\"CCSP\"].iloc[i+defaultWeek:i+pastWeek])\n",
    "        X_train.append(X.reshape(X.size))\n",
    "        Y_train.append(np.array(train.iloc[i+pastWeek:i+pastWeek+futureWeek][\"CCSP\"]))\n",
    "    return np.array(X_train), np.array(Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Min-max normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Use min-max normalization to scale the data to the range from 1 to 0\n",
    "sc = MinMaxScaler(feature_range = (0, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Design get_data() to get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(futureWeek):\n",
    "    \n",
    "    ## Read weekly copper price data\n",
    "    path = \"WeeklyFinalData.csv\"\n",
    "    data = read(path)\n",
    "    \n",
    "    date = data[\"Date\"]\n",
    "    data.drop(\"Date\", axis=1, inplace=True)\n",
    "    \n",
    "    ## Add time lag (pastWeek=4, futureWeek=1)\n",
    "    x_data, y_data = buildTrain(data, futureWeek=futureWeek)\n",
    "    \n",
    "    x_data1 = x_data[:157]\n",
    "    y_data1 = y_data[:157]\n",
    "    ## Split the data to training data and test data\n",
    "    x_train_data = x_data1[:int(x_data1.shape[0]*0.8)]\n",
    "    x_test_data = x_data1[int(x_data1.shape[0]*0.8):]\n",
    "    y_train_data = y_data1[:int(x_data1.shape[0]*0.8)]\n",
    "    y_test_data = y_data1[int(x_data1.shape[0]*0.8):]\n",
    "\n",
    "\n",
    "    return (x_train_data, x_test_data, y_train_data, y_test_data)\n",
    "\n",
    "#     return (x_data, y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(125, 18)\n",
      "(32, 18)\n",
      "(125, 4)\n",
      "(32, 4)\n"
     ]
    }
   ],
   "source": [
    "x_train_data, x_test_data, y_train_data, y_test_data = get_data(4)\n",
    "print(x_train_data.shape)\n",
    "print(x_test_data.shape)\n",
    "print(y_train_data.shape)\n",
    "print(y_test_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network():\n",
    "    \n",
    "    def __init__(self, nb_neuro, x_train_scaled, y_train_scaled):\n",
    "        \n",
    "        # Stop criteria - threshold\n",
    "        self.threshold_for_error = 0.07\n",
    "        self.threshold_for_lr = 1e-6\n",
    "        \n",
    "        # Input data \n",
    "        self.x = tf.convert_to_tensor(x_train_scaled, np.float32)\n",
    "        self.y = tf.convert_to_tensor(y_train_scaled, np.float32)\n",
    "        \n",
    "        # Learning rate\n",
    "        self.learning_rate = 1e-2\n",
    "        \n",
    "        \n",
    "        # Hidden layer I\n",
    "        self.n_neurons_in_h1 = nb_neuro\n",
    "        self.W1 = tf.Variable(tf.random.truncated_normal([self.x.shape[1], self.n_neurons_in_h1], mean=0, stddev=1))\n",
    "        self.b1 = tf.Variable(tf.random.truncated_normal([self.n_neurons_in_h1], mean=0, stddev=1))\n",
    "\n",
    "        # Output layer\n",
    "        self.Wo = tf.Variable(tf.random.truncated_normal([self.n_neurons_in_h1, self.y.shape[1]], mean=0, stddev=1))\n",
    "        self.bo = tf.Variable(tf.random.truncated_normal([self.y.shape[1]], mean=0, stddev=1))\n",
    "\n",
    "        # Whether the network is acceptable, default as False\n",
    "        self.acceptable = False\n",
    "        \n",
    "        # Some record for experiment\n",
    "        self.nb_node_pruned = 0\n",
    "        self.nb_node_acceptable=tf.convert_to_tensor([nb_neuro])\n",
    "        \n",
    "    \n",
    "    ## Forecast the test data\n",
    "    def forecast(self, x_test_scaled):\n",
    "    \n",
    "        x_test_scaled = tf.cast(x_test_scaled, tf.float32)\n",
    "        activation_value = tf.nn.relu((tf.matmul(x_test_scaled, self.W1)+self.b1))\n",
    "        forecast_value = tf.matmul(activation_value,self.Wo)+self.bo\n",
    "       \n",
    "        return forecast_value\n",
    "\n",
    "    ## Reset the x and y data\n",
    "    def setData(self, x_train_scaled, y_train_scaled):\n",
    "        self.x = tf.convert_to_tensor(x_train_scaled, np.float32)\n",
    "        self.y = tf.convert_to_tensor(y_train_scaled, np.float32)\n",
    "    \n",
    "    ## Add the new data to the x and y data\n",
    "    def addData(self, new_x_train, new_y_train):\n",
    "\n",
    "        self.x = tf.concat([self.x, new_x_train.reshape(1,-1)],0)\n",
    "        self.y = tf.concat([self.y, new_y_train.reshape(1,-1)],0)\n",
    "    \n",
    "    ## forward operation\n",
    "    def forward(self,  reg_strength= 0):\n",
    "        with tf.GradientTape() as tape:\n",
    "\n",
    "            y1 = tf.nn.relu((tf.matmul(self.x, self.W1)+self.b1))\n",
    "            yo = (tf.matmul(y1,self.Wo)+self.bo)\n",
    "\n",
    "            # performance measure\n",
    "            diff = yo-self.y\n",
    "            loss = tf.reduce_mean(diff**2) + (reg_strength/(self.Wo.shape[1]*(self.Wo.shape[0]+1)+self.W1.shape[1]*(self.W1.shape[0]+1))) * ((tf.nn.l2_loss(self.W1) + tf.nn.l2_loss(self.Wo) + tf.nn.l2_loss(self.b1) + tf.nn.l2_loss(self.bo))*2)\n",
    "\n",
    "        return(yo, loss, tape)\n",
    "\n",
    "    # backward operation\n",
    "    def backward_Adam(self,tape,loss):\n",
    "\n",
    "        optimizer = tf.optimizers.Adam(self.learning_rate)\n",
    "        gradients = tape.gradient(loss, [self.W1, self.Wo, self.b1, self.bo])\n",
    "        optimizer.apply_gradients(zip(gradients, [self.W1, self.Wo, self.b1, self.bo]))\n",
    "    \n",
    "    def backward_RMS(self,tape,loss):\n",
    "\n",
    "        optimizer = tf.keras.optimizers.RMSprop(self.learning_rate)\n",
    "        gradients = tape.gradient(loss, [self.W1, self.Wo, self.b1, self.bo])\n",
    "        optimizer.apply_gradients(zip(gradients, [self.W1, self.Wo, self.b1, self.bo]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing module (Check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initializing(network, initial_x, initial_y):\n",
    "    \n",
    "    ## Find each minimum output value y\n",
    "    min_y = tf.reduce_min(initial_y, axis=0)\n",
    "    \n",
    "    ## Subtract min_y from each y\n",
    "    res_y = initial_y-min_y\n",
    "    \n",
    "    ## Use linear regression to find the initial W1,b1,Wo,bo\n",
    "    reg = LinearRegression().fit(initial_x, res_y)\n",
    "\n",
    "    ## Set up the initial parameter of the network\n",
    "    network.W1 = tf.Variable(tf.cast(tf.transpose(reg.coef_), tf.float32))\n",
    "    network.b1 = tf.Variable(tf.convert_to_tensor(reg.intercept_, tf.float32))\n",
    "    network.Wo = tf.Variable(tf.convert_to_tensor([[1,0,0,0],[0,1,0,0],[0,0,1,0],[0,0,0,1]], tf.float32))\n",
    "    network.bo = tf.Variable(tf.cast(min_y, tf.float32))\n",
    "\n",
    "    ## Set up the acceptable of the initial network as True\n",
    "    network.acceptable =True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting module (Check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def selecting(network, x_train_scaled, y_train_scaled):\n",
    "    \n",
    "    \n",
    "    loss = []\n",
    "    temp_network = copy.deepcopy(network)\n",
    "    \n",
    "    ## Put each data into network to calculate the loss value\n",
    "    for i in range(x_train_scaled.shape[0]):\n",
    "        temp_network.setData(x_train_scaled[i].reshape(1,-1), y_train_scaled[i].reshape(1,-1))\n",
    "        loss.append((temp_network.forward()[1].numpy(),i))\n",
    "\n",
    "    ## Sort the data according to the loss value from smallest to largest, and save the data index in sorted_index\n",
    "    sorted_index = [sorted_data[1] for sorted_data in sorted(loss, key = lambda x:x[0])]\n",
    "\n",
    "    ## Print out some info for debug\n",
    "    print(\"The loss value of k:\",loss[sorted_index[0]])\n",
    "    print(\"Selecting module finish!\")\n",
    "    \n",
    "    return sorted_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matching module (Check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matching(network):\n",
    "\n",
    "    ## Set up the learning rate of the network\n",
    "    network.learning_rate = 1e-3\n",
    "    \n",
    "\n",
    "    while True:\n",
    "        \n",
    "        ## Get the loss value of the current network architecture\n",
    "        yo, loss, tape = network.forward()\n",
    "\n",
    "        ## Identify that all forecast value has met the error term\n",
    "        if tf.reduce_all(tf.math.abs(yo-network.y) <= network.threshold_for_error):\n",
    "            \n",
    "            ## If true, set the acceptable of the network as true and return it\n",
    "            network.acceptable = True\n",
    "            print(\"Matching finished - the network is acceptable\")\n",
    "            return(network)\n",
    "\n",
    "\n",
    "        ## If the error is not satisfied, continue to tunning the learning rate of the network\n",
    "        else:\n",
    "            \n",
    "            # Save the current papameter\n",
    "            network_pre = copy.deepcopy(network)\n",
    "            \n",
    "            # Stroe the last loss value\n",
    "            loss_pre = loss\n",
    "            \n",
    "            # Backward and check the loss performance of the network with new learning rate\n",
    "            network.backward_Adam(tape,loss)\n",
    "            yo, loss, tape = network.forward()\n",
    "\n",
    "            # Confirm whether the loss value of the adjusted network is smaller than the current one\n",
    "            if loss < loss_pre:\n",
    "\n",
    "                # If true, multiply the learning rate by 1.2\n",
    "                network.learning_rate *= 1.2\n",
    "\n",
    "            # On the contrary, reduce the learning rate\n",
    "            else:         \n",
    "                \n",
    "                # Identify whether the current learning rate is less than the threshold\n",
    "                if network.learning_rate <= network.threshold_for_lr:\n",
    "                    \n",
    "                    # If true, set the acceptable of the network as false and return it\n",
    "                    network.acceptable = False\n",
    "                    print(\"Matching finished - the network is Unacceptable\")\n",
    "                    return(network)\n",
    "\n",
    "                # On the contrary, restore w and adjust the learning rate\n",
    "                else:\n",
    "                    \n",
    "                    # Restore the papameter of the network\n",
    "                    network = network_pre\n",
    "                    network.learning_rate *= 0.7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cramming module (Check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cramming(network):\n",
    "    \n",
    "\n",
    "    ## Set the random seed\n",
    "    tf.random.set_seed(5)\n",
    "\n",
    "    ## Find unsatisfied data:K\n",
    "    yo, loss, tape = network.forward()\n",
    "    undesired_index = tf.where(tf.math.abs(yo-network.y) > network.threshold_for_error)\n",
    "\n",
    "    ## Print out the undesired_index for debug\n",
    "    print(\"The index of the undesired data:\",undesired_index)\n",
    "\n",
    "\n",
    "    ## Unsatisfied situation\n",
    "    for i in range(undesired_index.shape[0]):\n",
    "\n",
    "        ## Find the index of the unsatisfied data\n",
    "        k_data_num = undesired_index[i][0]\n",
    "        k_l = undesired_index[i][1]\n",
    "\n",
    "        undesired_data = tf.reshape(network.x[k_data_num,:], [1,-1])\n",
    "\n",
    "        ## Remove the data that does not meet the error term\n",
    "        left_data = network.x[:k_data_num,:]\n",
    "        right_data = network.x[k_data_num+1:,:]\n",
    "        remain_tensor = tf.concat([left_data, right_data], 0)\n",
    "\n",
    "        \n",
    "        ## Use the random method to find out the gamma and zeta\n",
    "        while True:\n",
    "\n",
    "            ## Find m-vector gamma: r\n",
    "            ## Use the random method to generate the gamma that can make the conditions met\n",
    "            gamma = tf.random.uniform(shape=[1,network.x.shape[1]])\n",
    "            subtract_undesired_data = tf.subtract(remain_tensor, undesired_data)\n",
    "            matmul_value = tf.matmul(gamma,tf.transpose(subtract_undesired_data))\n",
    "\n",
    "\n",
    "            if tf.reduce_all(matmul_value != 0):\n",
    "\n",
    "                while True:\n",
    "\n",
    "                    ## Find the tiny value: zeta\n",
    "                    ## Use the random method to generate the zeta that can make the conditions met\n",
    "                    zeta = tf.random.uniform(shape=[1])\n",
    "\n",
    "                    if tf.reduce_all(tf.multiply(tf.add(zeta,matmul_value),tf.subtract(zeta,matmul_value))<0):\n",
    "                        break\n",
    "\n",
    "                break\n",
    "\n",
    "\n",
    "        ## The weight of input layer to hidden layer I\n",
    "        w10 = gamma\n",
    "        w11 = gamma\n",
    "        w12 = gamma\n",
    "        \n",
    "        W1_new = tf.transpose(tf.concat([w10,w11,w12],0))\n",
    "\n",
    "        ## The bias of input layer to hidden layer I\n",
    "        matual_value = tf.matmul(gamma,tf.transpose(undesired_data))\n",
    "\n",
    "        b10 = tf.subtract(zeta,matual_value)\n",
    "        b11 = -1*matual_value\n",
    "        b12 = tf.subtract(-1*zeta,matual_value)\n",
    "        b1_new = tf.reshape(tf.concat([b10,b11,b12],0),[3])\n",
    "\n",
    "        ## The weight of hidden layer I to output layer\n",
    "        gap = network.y[k_data_num, k_l]-yo[k_data_num, k_l]\n",
    "\n",
    "        wo0_value = gap/zeta\n",
    "        wo1_value = (-2*gap)/zeta\n",
    "        wo2_value = gap/zeta\n",
    "\n",
    "        wo0 = tf.reshape(tf.one_hot(k_l,4,dtype=tf.float32) * wo0_value, shape=(1,-1))\n",
    "        wo1 = tf.reshape(tf.one_hot(k_l,4,dtype=tf.float32) * wo1_value, shape=(1,-1))\n",
    "        wo2 = tf.reshape(tf.one_hot(k_l,4,dtype=tf.float32) * wo2_value, shape=(1,-1))\n",
    "\n",
    "        Wo_new = tf.concat([wo0,wo1,wo2],0)\n",
    "\n",
    "        ## Add new neuroes to the network\n",
    "        network.W1 = tf.Variable(tf.concat([network.W1, W1_new],1), tf.float32)\n",
    "        network.b1 = tf.Variable(tf.concat([network.b1, b1_new],0), tf.float32)\n",
    "        network.Wo = tf.Variable(tf.concat([network.Wo, Wo_new],0), tf.float32)\n",
    "\n",
    "        yo, loss, tape = network.forward()\n",
    "   \n",
    "        ## Determine if cramming is successful and print out the corresponding information\n",
    "        if tf.reduce_all(tf.math.abs(yo[k_data_num,k_l]-network.y[k_data_num,k_l]) <= network.threshold_for_error):\n",
    "            \n",
    "            ## If the cramming process is complete, set the acceptable of the network as true\n",
    "            if i==(undesired_index.shape[0]-1):\n",
    "                network.acceptable = True\n",
    "            \n",
    "            print(\"Cramming success!\")\n",
    "            \n",
    "        \n",
    "        else:\n",
    "            print(\"Cramming failed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularizing module (Check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regularizing(network):\n",
    "\n",
    "    ## Record the number of executions\n",
    "    times_enlarge = 0\n",
    "    times_shrink = 0\n",
    "    ## Set up the learning rate of the network\n",
    "    network.learning_rate = 1e-3\n",
    "\n",
    "    ## Set epoch to 100\n",
    "    for i in range(100):\n",
    "\n",
    "        ## Store the parameter of the network\n",
    "        network_pre = copy.deepcopy(network)\n",
    "        yo, loss, tape = network.forward(1e-2)\n",
    "        loss_pre = loss\n",
    "\n",
    "        ## Backward operation to obtain w'\n",
    "        network.backward_Adam(tape, loss)\n",
    "        yo, loss, tape = network.forward(1e-2)\n",
    "\n",
    "         # Confirm whether the adjusted loss value is smaller than the current one\n",
    "        if loss <= loss_pre:\n",
    "            \n",
    "            ## Identify that all forecast value has met the error term\n",
    "            if tf.reduce_all(tf.math.abs(yo-network.y) <= network.threshold_for_error):\n",
    "                \n",
    "                ## If true, multiply the learning rate by 1.2\n",
    "                network.learning_rate *= 1.2\n",
    "                times_enlarge += 1\n",
    "#                 print(\"Regularizing process\")\n",
    "\n",
    "            else:\n",
    "\n",
    "                ## Else, restore w and end the process\n",
    "                network = network_pre\n",
    "                print(\"Number of enlarge:\",times_enlarge)\n",
    "                print(\"Number of shrink:\",times_shrink)\n",
    "                print(\"Regularizing result: Unable to meet the error term\")\n",
    "                return(network)\n",
    "\n",
    "        # If the adjusted loss value is not smaller than the current one\n",
    "        else:\n",
    "           \n",
    "\n",
    "            ## If the learning rate is greater than the threshold for learning rate\n",
    "            if network.learning_rate > network.threshold_for_lr:\n",
    "                \n",
    "                ## Restore the w and multiply the learning rate by 0.7\n",
    "                network = network_pre\n",
    "                network.learning_rate *= 0.7\n",
    "                times_shrink += 1\n",
    "             ## If the learning rate is smaller than the threshold for learning rate\n",
    "            else:\n",
    "                \n",
    "                ## Restore the w\n",
    "                network = network_pre\n",
    "                print(\"Number of enlarge:\",times_enlarge)\n",
    "                print(\"Number of shrink:\",times_shrink)\n",
    "                print(\"Regularizing result: Less than the epsilon for the learning rate\")\n",
    "                return(network)\n",
    "\n",
    "        if i == 99:\n",
    "            print(\"Number of enlarge:\",times_enlarge)\n",
    "            print(\"Number of shrink:\",times_shrink)\n",
    "            print(\"Regularizing result: The number of rounds has reached\")\n",
    "            return(network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reorganizing module (Check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reorganizing(network):\n",
    "    \n",
    "    ## Set up the k = 1, and p = the number of hidden node\n",
    "    k = 1\n",
    "    p = network.W1.shape[1]\n",
    "\n",
    "    while True:\n",
    "\n",
    "        ## If k > p, end of Process\n",
    "        if k > p:\n",
    "\n",
    "            print(\"Reorganizing result: The final number of neuro is \",p)\n",
    "            return(network)\n",
    "\n",
    "        ## Else, Process is ongoing\n",
    "        else:\n",
    "\n",
    "            ## Using the regularizing module to adjust the network\n",
    "            network = regularizing(network)\n",
    "            \n",
    "            ## Store the network and w\n",
    "            network_pre = copy.deepcopy(network)\n",
    "\n",
    "            ## Set up the acceptable of the network as false\n",
    "            network.acceptable = False\n",
    "\n",
    "            ## Ignore the K hidden node\n",
    "            network.W1 = tf.Variable(tf.concat([network.W1[:,:k-1],network.W1[:,k:]],1))\n",
    "            network.b1 = tf.Variable(tf.concat([network.b1[:k-1],network.b1[k:]],0))\n",
    "            network.Wo = tf.Variable(tf.concat([network.Wo[:k-1,:],network.Wo[k:,:]],0))\n",
    "\n",
    "            ## Using the matching module to adjust the network\n",
    "            network = matching(network)\n",
    "\n",
    "            ## If the resulting network is acceptable, this means that the k hidden node can be removed\n",
    "            if network.acceptable:\n",
    "\n",
    "                print(\"Drop out the nero number: %d / %d\" %(k, p))\n",
    "                network.nb_node_pruned += 1\n",
    "                ## p--\n",
    "                p-=1\n",
    "\n",
    "            ## Else, it means that the k hidden node cannot be removed\n",
    "            else:\n",
    "                \n",
    "                ## Restore the network and w\n",
    "                network = network_pre\n",
    "                print(\"Cannot drop out the nero number: %d / %d\" %(k, p))\n",
    "                \n",
    "                ## k++\n",
    "                k+=1\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Call the help method \"get_data(...)\" to get the training data and test data \n",
    "x_train_data, x_test_data, y_train_data, y_test_data = get_data(4)\n",
    "\n",
    "## Use min-max normalization to normalize data in a range of 0 to 1\n",
    "x_train_scaled = sc.fit_transform(x_train_data)\n",
    "x_test_scaled = sc.transform(x_test_data)\n",
    "y_train_scaled = sc.fit_transform(y_train_data)\n",
    "\n",
    "## Use the first 25 training data to debug\n",
    "x_train_scaled = x_train_scaled[:22]\n",
    "y_train_scaled = y_train_scaled[:22]\n",
    "\n",
    "## Pick up m+1 data that are linearly independent as the initial m+1 training data \n",
    "initial_x = x_train_scaled[:x_train_scaled.shape[1]+1]\n",
    "initial_y = y_train_scaled[:x_train_scaled.shape[1]+1]\n",
    "\n",
    "## Construct the network object with 4 neuros\n",
    "network = Network(4, initial_x, initial_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network.b1.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct a instance of network\n",
    "- trained through the matching module, reorganizing module, and cramming module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data index: 20\n",
      "The loss value of k: (0.12238025, 0)\n",
      "Selecting module finish!\n",
      "The error term for each data\n",
      "tf.Tensor(\n",
      "[[0.00000125 0.0000059  0.         0.00000048]\n",
      " [0.00000054 0.00000352 0.00000036 0.00000042]\n",
      " [0.00000471 0.00000358 0.00000024 0.00000012]\n",
      " [0.         0.00000161 0.00000036 0.00000036]\n",
      " [0.00000149 0.00000274 0.00000054 0.0000006 ]\n",
      " [0.00000036 0.00000083 0.00000024 0.0000006 ]\n",
      " [0.00000107 0.00000346 0.00000006 0.00000024]\n",
      " [0.00000012 0.00000107 0.00000024 0.00000036]\n",
      " [0.00000131 0.00000048 0.00000006 0.00000054]\n",
      " [0.00000215 0.00000179 0.00000012 0.00000077]\n",
      " [0.00000203 0.00000077 0.0000003  0.00000036]\n",
      " [0.00000054 0.00000089 0.00000024 0.00000018]\n",
      " [0.00000113 0.0000006  0.00000018 0.00000042]\n",
      " [0.00000036 0.00000042 0.00000054 0.00000054]\n",
      " [0.00000364 0.00000018 0.00000054 0.0000003 ]\n",
      " [0.00000578 0.0000003  0.0000003  0.00000036]\n",
      " [0.00000006 0.00000328 0.00000012 0.00000048]\n",
      " [0.0000003  0.00000799 0.00000012 0.00000012]\n",
      " [0.00000441 0.0000031  0.00000024 0.00000018]\n",
      " [0.64943933 0.14667118 0.19006425 0.1005497 ]], shape=(20, 4), dtype=float32)\n",
      "Matching finished - the network is acceptable\n",
      "Number of enlarge: 0\n",
      "Number of shrink: 0\n",
      "Regularizing result: Unable to meet the error term\n",
      "Matching finished - the network is Unacceptable\n",
      "Cannot drop out the nero number: 1 / 4\n",
      "Number of enlarge: 0\n",
      "Number of shrink: 0\n",
      "Regularizing result: Unable to meet the error term\n",
      "Matching finished - the network is acceptable\n",
      "Drop out the nero number: 2 / 4\n",
      "Number of enlarge: 0\n",
      "Number of shrink: 0\n",
      "Regularizing result: Unable to meet the error term\n",
      "Matching finished - the network is Unacceptable\n",
      "Cannot drop out the nero number: 2 / 3\n",
      "Number of enlarge: 0\n",
      "Number of shrink: 0\n",
      "Regularizing result: Unable to meet the error term\n",
      "Matching finished - the network is Unacceptable\n",
      "Cannot drop out the nero number: 3 / 3\n",
      "Reorganizing result: The final number of neuro is  3\n",
      "The network status: True\n",
      "---------- next data ----------\n",
      "The data index: 21\n",
      "The loss value of k: (0.009389008, 69)\n",
      "Selecting module finish!\n",
      "The error term for each data\n",
      "tf.Tensor(\n",
      "[[0.04601556 0.06999707 0.0139299  0.00885302]\n",
      " [0.04507327 0.02956778 0.02712333 0.00800848]\n",
      " [0.01489604 0.02921182 0.00819242 0.00275576]\n",
      " [0.03348434 0.01560915 0.03320491 0.00627869]\n",
      " [0.02192342 0.03350186 0.03179622 0.01571506]\n",
      " [0.01383442 0.05836916 0.00091076 0.00076765]\n",
      " [0.05496997 0.02330935 0.05195403 0.01901746]\n",
      " [0.0026269  0.04905349 0.01133835 0.00529897]\n",
      " [0.00450623 0.02206278 0.02378416 0.00205189]\n",
      " [0.01472276 0.02237558 0.01116389 0.0005284 ]\n",
      " [0.02930331 0.01856256 0.02761644 0.00555789]\n",
      " [0.00925344 0.02262968 0.0087682  0.00300866]\n",
      " [0.01379031 0.01663512 0.02035433 0.01071304]\n",
      " [0.01201558 0.02409929 0.01224369 0.00453705]\n",
      " [0.00152761 0.01859051 0.00531673 0.0012359 ]\n",
      " [0.00274199 0.01574486 0.00551701 0.0046314 ]\n",
      " [0.011558   0.00635105 0.00526613 0.00082648]\n",
      " [0.00796705 0.00070047 0.03500229 0.00442171]\n",
      " [0.03178978 0.00585854 0.05400169 0.01090086]\n",
      " [0.01893085 0.00028241 0.01974165 0.00351447]\n",
      " [0.07939196 0.01528674 0.0399943  0.17152178]], shape=(21, 4), dtype=float32)\n",
      "Matching finished - the network is acceptable\n",
      "Number of enlarge: 3\n",
      "Number of shrink: 1\n",
      "Regularizing result: Unable to meet the error term\n",
      "Matching finished - the network is acceptable\n",
      "Drop out the nero number: 1 / 3\n",
      "Number of enlarge: 0\n",
      "Number of shrink: 0\n",
      "Regularizing result: Unable to meet the error term\n",
      "Matching finished - the network is Unacceptable\n",
      "Cannot drop out the nero number: 1 / 2\n",
      "Number of enlarge: 0\n",
      "Number of shrink: 0\n",
      "Regularizing result: Unable to meet the error term\n",
      "Matching finished - the network is Unacceptable\n",
      "Cannot drop out the nero number: 2 / 2\n",
      "Reorganizing result: The final number of neuro is  2\n",
      "The network status: True\n",
      "---------- next data ----------\n",
      "The data index: 22\n",
      "The loss value of k: (0.0010808029, 69)\n",
      "Selecting module finish!\n",
      "The error term for each data\n",
      "tf.Tensor(\n",
      "[[0.04047108 0.06378061 0.00200671 0.02060986]\n",
      " [0.06999904 0.03081757 0.029046   0.02004451]\n",
      " [0.01795024 0.04081255 0.01733202 0.0188055 ]\n",
      " [0.02859455 0.01442027 0.02164769 0.00119799]\n",
      " [0.03605133 0.03669345 0.04835594 0.01928914]\n",
      " [0.00726432 0.05696428 0.00313175 0.01599324]\n",
      " [0.06955874 0.0306381  0.0492326  0.01990521]\n",
      " [0.00956243 0.04627836 0.01895559 0.01399231]\n",
      " [0.02509993 0.00147742 0.02713215 0.0218057 ]\n",
      " [0.01465666 0.02872211 0.00044107 0.00599766]\n",
      " [0.03323084 0.01617181 0.02494705 0.00106508]\n",
      " [0.02293557 0.02454311 0.01632202 0.00120002]\n",
      " [0.00545305 0.01406008 0.01655942 0.01082903]\n",
      " [0.02819926 0.00539494 0.01925933 0.0089367 ]\n",
      " [0.01417023 0.01242095 0.00904554 0.01191062]\n",
      " [0.00911081 0.00864601 0.0061242  0.00118488]\n",
      " [0.02430761 0.00930279 0.01279515 0.00196487]\n",
      " [0.00043392 0.01517367 0.04147017 0.0113318 ]\n",
      " [0.02322    0.02704054 0.05437338 0.03120071]\n",
      " [0.06960964 0.03514957 0.00744253 0.01287228]\n",
      " [0.03019649 0.0219487  0.03131145 0.02989686]\n",
      " [0.02339911 0.00317067 0.02467877 0.05618373]], shape=(22, 4), dtype=float32)\n",
      "Number of enlarge: 0\n",
      "Number of shrink: 0\n",
      "Regularizing result: Unable to meet the error term\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "nb_step4 = 0\n",
    "nb_step6_1 = 0\n",
    "nb_step6_2 = 0\n",
    "\n",
    "## Call the help method \"get_data(...)\" to get the training data and test data \n",
    "x_train_data, x_test_data, y_train_data, y_test_data = get_data(4)\n",
    "\n",
    "## Use min-max normalization to normalize data in a range of 0 to 1\n",
    "x_train_scaled = sc.fit_transform(x_train_data)\n",
    "x_test_scaled = sc.transform(x_test_data)\n",
    "y_train_scaled = sc.fit_transform(y_train_data)\n",
    "\n",
    "## Use the first 25 training data to debug\n",
    "# x_train_scaled = x_train_scaled[:22]\n",
    "# y_train_scaled = y_train_scaled[:22]\n",
    "\n",
    "## Pick up m+1 data that are linearly independent as the initial m+1 training data \n",
    "initial_x = x_train_scaled[:x_train_scaled.shape[1]+1]\n",
    "initial_y = y_train_scaled[:x_train_scaled.shape[1]+1]\n",
    "\n",
    "## The remaining data (that exclude initial data)\n",
    "x_train_scaled = x_train_scaled[x_train_scaled.shape[1]+1:]\n",
    "y_train_scaled = y_train_scaled[x_train_scaled.shape[1]+1:]\n",
    "\n",
    "## Construct the network object with 4 neuros\n",
    "network = Network(4, initial_x, initial_y)\n",
    "\n",
    "## Use initilize module to set up the initial network\n",
    "initializing(network, initial_x, initial_y)\n",
    "\n",
    "\n",
    "## Training of all data\n",
    "for i in range(0, x_train_scaled.shape[0]):\n",
    "    \n",
    "   \n",
    "    ## Print out some info for debug\n",
    "    print(\"The data index: %d\"%(i+x_train_scaled.shape[1]+2))\n",
    "    \n",
    "    ## Get the data index by selecting module to sort the data by the loss value from smallest to largest\n",
    "    sorted_index = selecting(network, x_train_scaled, y_train_scaled)\n",
    "    \n",
    "    ## Add new data for training\n",
    "    network.addData(x_train_scaled[sorted_index[0]], y_train_scaled[sorted_index[0]])\n",
    "    x_train_scaled = np.delete(x_train_scaled, sorted_index[0], 0)\n",
    "    y_train_scaled = np.delete(y_train_scaled, sorted_index[0], 0)\n",
    "    \n",
    "    yo, loss, tape = network.forward()\n",
    "    \n",
    "    ## Print out some information for debug\n",
    "    print(\"The error term for each data\")\n",
    "    print(tf.math.abs(yo-network.y))\n",
    "    \n",
    "    ## Determine whether the forecast value can meet the error term\n",
    "    if tf.reduce_all(tf.math.abs(yo-network.y) <= network.threshold_for_error):\n",
    "        \n",
    "        ## If true, set up the acceptable of the network as true\n",
    "        network.acceptable = True\n",
    "        \n",
    "        ## Use reorganizing module to adjust the model\n",
    "        network = reorganizing(network)\n",
    "        \n",
    "        ## Record the number of runs\n",
    "        nb_step4 += 1\n",
    " \n",
    "    else:\n",
    "        \n",
    "        ## If true, set up the acceptable of the network as false\n",
    "        network.acceptable = False\n",
    "        network_pre = copy.deepcopy(network)\n",
    "        \n",
    "        ## Use matching module to adjust the model\n",
    "        network = matching(network)\n",
    "        \n",
    "        ## If the output of the matching module is an acceptable network, use the reorganization module to adjust the model\n",
    "        if network.acceptable:\n",
    "            network = reorganizing(network)\n",
    "            \n",
    "            ## Record the number of runs\n",
    "            nb_step6_1 += 1\n",
    " \n",
    "        ## Else (if the output of the matching module is an unacceptable network)\n",
    "        else:\n",
    "            network = network_pre\n",
    "            \n",
    "            ## Use cramming module and reorganizing module to adjust the model\n",
    "            cramming(network)\n",
    "            network = reorganizing(network)\n",
    "            \n",
    "            ## Record the number of runs\n",
    "            nb_step6_2 += 1\n",
    "\n",
    "    ## Print out the model status\n",
    "    network.nb_node_acceptable = tf.concat([network.nb_node_acceptable, [network.b1.shape[0]]],0)\n",
    "    print(\"The network status:\",network.acceptable)\n",
    "    print(\"-\"*10,\"next data\",\"-\"*10)\n",
    "## Calculate the training time    \n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_value = network.forecast(x_test_scaled)\n",
    "\n",
    "y_pred = sc.inverse_transform(forecast_value)\n",
    "accuracy = list()\n",
    "\n",
    "for i in range(y_pred.shape[1]):\n",
    "#     for _ in range(y_pred.shape[0]): \n",
    "\n",
    "    correct_times = np.sum(tf.math.abs(y_test_data[:,i]-y_pred[:,i])<2000)\n",
    "    accuracy.append(correct_times/y_pred.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The training time(s):\",end - start)\n",
    "\n",
    "print(\"-\"*30)\n",
    "total_time = nb_step4 + nb_step6_1 + nb_step6_2\n",
    "print(\"The percentage of each step\")\n",
    "print(\"Step 4:%.2f\"%(nb_step4/total_time))\n",
    "print(\"Step 6.1:%.2f\"%(nb_step6_1/total_time))\n",
    "print(\"Step 6.2:%.2f\"%(nb_step6_2/total_time))\n",
    "\n",
    "print(\"-\"*30)\n",
    "print(\"Total amount of cramming occurrences:\",nb_step6_2)\n",
    "\n",
    "print(\"-\"*30)\n",
    "print(\"The amount of hidden node that be pruned:\",network.nb_node_pruned)\n",
    "\n",
    "print(\"-\"*30)\n",
    "print(\"The amount of adopted hidden nodes:\",network.nb_node_acceptable[-1].numpy())\n",
    "\n",
    "print(\"-\"*30)\n",
    "print(\"Accuracy\")\n",
    "print(\"The accuracy for the t+1:\",accuracy[0])\n",
    "print(\"The accuracy for the t+2:\",accuracy[1])\n",
    "print(\"The accuracy for the t+3:\",accuracy[2])\n",
    "print(\"The accuracy for the t+4:\",accuracy[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yo, loss, tape = network.forward()\n",
    "plt.plot(sc.inverse_transform(yo)[:,0], label=\"LLAAT\")\n",
    "plt.plot(sc.inverse_transform(network.y)[:,0], label=\"Actual\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forecast_value = network.forecast(x_test_scaled)\n",
    "plt.plot(sc.inverse_transform(forecast_value)[:,0], label=\"LLAAT\")\n",
    "plt.plot(y_test_data[:,0], label=\"Actual\")\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
