{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import related package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Enabling eager execution\n",
      "INFO:tensorflow:Enabling v2 tensorshape\n",
      "INFO:tensorflow:Enabling resource variables\n",
      "INFO:tensorflow:Enabling tensor equality\n",
      "INFO:tensorflow:Enabling control flow v2\n"
     ]
    }
   ],
   "source": [
    "## Import tensorflow package for modeling\n",
    "import tensorflow as tf\n",
    "\n",
    "## Data processing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "## Min-max normalization\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "## Plot the graph\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "## Initializing module\n",
    "from sklearn.linear_model import LinearRegression\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "## Copy module\n",
    "import copy\n",
    "\n",
    "## Used to calculate the training time\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Control memory usage space for GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_options = tf.compat.v1.GPUOptions(per_process_gpu_memory_fraction=0.3)\n",
    "sess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(gpu_options=gpu_options))\n",
    "tf.compat.v1.keras.backend.set_session(sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read(path):\n",
    "    return pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildTrain(train, pastWeek=4, futureWeek=4, defaultWeek=1):\n",
    "    X_train, Y_train = [], []\n",
    "    for i in range(train.shape[0]-futureWeek-pastWeek):\n",
    "        X = np.array(train.iloc[i:i+defaultWeek])\n",
    "        X = np.append(X,train[\"CCSP\"].iloc[i+defaultWeek:i+pastWeek])\n",
    "        X_train.append(X.reshape(X.size))\n",
    "        Y_train.append(np.array(train.iloc[i+pastWeek:i+pastWeek+futureWeek][\"CCSP\"]))\n",
    "    return np.array(X_train), np.array(Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Min-max normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Use min-max normalization to scale the data to the range from 1 to 0\n",
    "sc = MinMaxScaler(feature_range = (0, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Design get_data() to get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(futureWeek):\n",
    "    \n",
    "    ## Read weekly copper price data\n",
    "    path = \"WeeklyFinalData.csv\"\n",
    "    data = read(path)\n",
    "    \n",
    "    date = data[\"Date\"]\n",
    "    data.drop(\"Date\", axis=1, inplace=True)\n",
    "    \n",
    "    ## Add time lag (pastWeek=4, futureWeek=1)\n",
    "    x_data, y_data = buildTrain(data, futureWeek=futureWeek)\n",
    "    \n",
    "    x_data1 = x_data[:157]\n",
    "    y_data1 = y_data[:157]\n",
    "    ## Split the data to training data and test data\n",
    "    x_train_data = x_data[:int(x_data.shape[0]*0.8)]\n",
    "    x_test_data = x_data[int(x_data.shape[0]*0.8):]\n",
    "    y_train_data = y_data[:int(x_data.shape[0]*0.8)]\n",
    "    y_test_data = y_data[int(x_data.shape[0]*0.8):]\n",
    "\n",
    "\n",
    "    return (x_train_data, x_test_data, y_train_data, y_test_data)\n",
    "\n",
    "#     return (x_data, y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2,), dtype=int32, numpy=array([2, 2], dtype=int32)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = tf.convert_to_tensor([2])\n",
    "tf.concat([a, [2]],0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network():\n",
    "    \n",
    "    def __init__(self, nb_neuro, x_train_scaled, y_train_scaled):\n",
    "        \n",
    "        # Stop criteria - threshold\n",
    "        self.threshold_for_error = 0.07\n",
    "        self.threshold_for_lr = 1e-6\n",
    "        \n",
    "        # Input data \n",
    "        self.x = tf.convert_to_tensor(x_train_scaled, np.float32)\n",
    "        self.y = tf.convert_to_tensor(y_train_scaled, np.float32)\n",
    "        \n",
    "        # Learning rate\n",
    "        self.learning_rate = 1e-2\n",
    "        \n",
    "        \n",
    "        # Hidden layer I\n",
    "        self.n_neurons_in_h1 = nb_neuro\n",
    "        self.W1 = tf.Variable(tf.random.truncated_normal([self.x.shape[1], self.n_neurons_in_h1], mean=0, stddev=1))\n",
    "        self.b1 = tf.Variable(tf.random.truncated_normal([self.n_neurons_in_h1], mean=0, stddev=1))\n",
    "\n",
    "        # Output layer\n",
    "        self.Wo = tf.Variable(tf.random.truncated_normal([self.n_neurons_in_h1, self.y.shape[1]], mean=0, stddev=1))\n",
    "        self.bo = tf.Variable(tf.random.truncated_normal([self.y.shape[1]], mean=0, stddev=1))\n",
    "\n",
    "        # Whether the network is acceptable, default as False\n",
    "        self.acceptable = False\n",
    "        \n",
    "        # Some record for experiment\n",
    "        self.nb_node_pruned = 0\n",
    "        self.nb_node_acceptable=tf.convert_to_tensor([nb_neuro])\n",
    "        \n",
    "    \n",
    "    ## Forecast the test data\n",
    "    def forecast(self, x_test_scaled):\n",
    "    \n",
    "        x_test_scaled = tf.cast(x_test_scaled, tf.float32)\n",
    "        activation_value = tf.nn.relu((tf.matmul(x_test_scaled, self.W1)+self.b1))\n",
    "        forecast_value = tf.matmul(activation_value,self.Wo)+self.bo\n",
    "       \n",
    "        return forecast_value\n",
    "\n",
    "    ## Reset the x and y data\n",
    "    def setData(self, x_train_scaled, y_train_scaled):\n",
    "        self.x = tf.convert_to_tensor(x_train_scaled, np.float32)\n",
    "        self.y = tf.convert_to_tensor(y_train_scaled, np.float32)\n",
    "    \n",
    "    ## Add the new data to the x and y data\n",
    "    def addData(self, new_x_train, new_y_train):\n",
    "\n",
    "        self.x = tf.concat([self.x, new_x_train.reshape(1,-1)],0)\n",
    "        self.y = tf.concat([self.y, new_y_train.reshape(1,-1)],0)\n",
    "    \n",
    "    ## forward operation\n",
    "    def forward(self,  reg_strength= 0):\n",
    "        with tf.GradientTape() as tape:\n",
    "\n",
    "            y1 = tf.nn.relu((tf.matmul(self.x, self.W1)+self.b1))\n",
    "            yo = (tf.matmul(y1,self.Wo)+self.bo)\n",
    "\n",
    "            # performance measure\n",
    "            diff = yo-self.y\n",
    "            loss = tf.reduce_mean(diff**2) + (reg_strength/(self.Wo.shape[1]*(self.Wo.shape[0]+1)+self.W1.shape[1]*(self.W1.shape[0]+1))) * ((tf.nn.l2_loss(self.W1) + tf.nn.l2_loss(self.Wo) + tf.nn.l2_loss(self.b1) + tf.nn.l2_loss(self.bo))*2)\n",
    "\n",
    "        return(yo, loss, tape)\n",
    "\n",
    "    # backward operation\n",
    "    def backward_Adam(self,tape,loss):\n",
    "\n",
    "        optimizer = tf.optimizers.Adam(self.learning_rate)\n",
    "        gradients = tape.gradient(loss, [self.W1, self.Wo, self.b1, self.bo])\n",
    "        optimizer.apply_gradients(zip(gradients, [self.W1, self.Wo, self.b1, self.bo]))\n",
    "    \n",
    "    def backward_RMS(self,tape,loss):\n",
    "\n",
    "        optimizer = tf.keras.optimizers.RMSprop(self.learning_rate)\n",
    "        gradients = tape.gradient(loss, [self.W1, self.Wo, self.b1, self.bo])\n",
    "        optimizer.apply_gradients(zip(gradients, [self.W1, self.Wo, self.b1, self.bo]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing module (Check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initializing(network, initial_x, initial_y):\n",
    "    \n",
    "    ## Find each minimum output value y\n",
    "    min_y = tf.reduce_min(initial_y, axis=0)\n",
    "    \n",
    "    ## Subtract min_y from each y\n",
    "    res_y = initial_y-min_y\n",
    "    \n",
    "    ## Use linear regression to find the initial W1,b1,Wo,bo\n",
    "    reg = LinearRegression().fit(initial_x, res_y)\n",
    "\n",
    "    ## Set up the initial parameter of the network\n",
    "    network.W1 = tf.Variable(tf.cast(tf.transpose(reg.coef_), tf.float32))\n",
    "    network.b1 = tf.Variable(tf.convert_to_tensor(reg.intercept_, tf.float32))\n",
    "    network.Wo = tf.Variable(tf.convert_to_tensor([[1,0,0,0],[0,1,0,0],[0,0,1,0],[0,0,0,1]], tf.float32))\n",
    "    network.bo = tf.Variable(tf.cast(min_y, tf.float32))\n",
    "\n",
    "    ## Set up the acceptable of the initial network as True\n",
    "    network.acceptable =True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting module (Check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def selecting(network, x_train_scaled, y_train_scaled):\n",
    "    \n",
    "    \n",
    "    loss = []\n",
    "    temp_network = copy.deepcopy(network)\n",
    "    \n",
    "    ## Put each data into network to calculate the loss value\n",
    "    for i in range(x_train_scaled.shape[0]):\n",
    "        temp_network.setData(x_train_scaled[i].reshape(1,-1), y_train_scaled[i].reshape(1,-1))\n",
    "        loss.append((temp_network.forward()[1].numpy(),i))\n",
    "\n",
    "    ## Sort the data according to the loss value from smallest to largest, and save the data index in sorted_index\n",
    "    sorted_index = [sorted_data[1] for sorted_data in sorted(loss, key = lambda x:x[0])]\n",
    "\n",
    "    ## Print out some info for debug\n",
    "    print(\"The loss value of k:\",loss[sorted_index[0]])\n",
    "    print(\"Selecting module finish!\")\n",
    "    \n",
    "    return sorted_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matching module (Check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matching(network):\n",
    "\n",
    "    ## Set up the learning rate of the network\n",
    "    network.learning_rate = 1e-3\n",
    "    \n",
    "\n",
    "    while True:\n",
    "        \n",
    "        ## Get the loss value of the current network architecture\n",
    "        yo, loss, tape = network.forward()\n",
    "\n",
    "        ## Identify that all forecast value has met the error term\n",
    "        if tf.reduce_all(tf.math.abs(yo-network.y) <= network.threshold_for_error):\n",
    "            \n",
    "            ## If true, set the acceptable of the network as true and return it\n",
    "            network.acceptable = True\n",
    "            print(\"Matching finished - the network is acceptable\")\n",
    "            return(network)\n",
    "\n",
    "\n",
    "        ## If the error is not satisfied, continue to tunning the learning rate of the network\n",
    "        else:\n",
    "            \n",
    "            # Save the current papameter\n",
    "            network_pre = copy.deepcopy(network)\n",
    "            \n",
    "            # Stroe the last loss value\n",
    "            loss_pre = loss\n",
    "            \n",
    "            # Backward and check the loss performance of the network with new learning rate\n",
    "            network.backward_Adam(tape,loss)\n",
    "            yo, loss, tape = network.forward()\n",
    "\n",
    "            # Confirm whether the loss value of the adjusted network is smaller than the current one\n",
    "            if loss < loss_pre:\n",
    "\n",
    "                # If true, multiply the learning rate by 1.2\n",
    "                network.learning_rate *= 1.2\n",
    "\n",
    "            # On the contrary, reduce the learning rate\n",
    "            else:         \n",
    "                \n",
    "                # Identify whether the current learning rate is less than the threshold\n",
    "                if network.learning_rate <= network.threshold_for_lr:\n",
    "                    \n",
    "                    # If true, set the acceptable of the network as false and return it\n",
    "                    network.acceptable = False\n",
    "                    print(\"Matching finished - the network is Unacceptable\")\n",
    "                    return(network)\n",
    "\n",
    "                # On the contrary, restore w and adjust the learning rate\n",
    "                else:\n",
    "                    \n",
    "                    # Restore the papameter of the network\n",
    "                    network = network_pre\n",
    "                    network.learning_rate *= 0.7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cramming module (Check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cramming(network):\n",
    "    \n",
    "\n",
    "    ## Set the random seed\n",
    "    tf.random.set_seed(5)\n",
    "\n",
    "    ## Find unsatisfied data:K\n",
    "    yo, loss, tape = network.forward()\n",
    "    undesired_index = tf.where(tf.math.abs(yo-network.y) > network.threshold_for_error)\n",
    "\n",
    "    ## Print out the undesired_index for debug\n",
    "    print(\"The index of the undesired data:\",undesired_index)\n",
    "\n",
    "\n",
    "    ## Unsatisfied situation\n",
    "    for i in range(undesired_index.shape[0]):\n",
    "\n",
    "        ## Find the index of the unsatisfied data\n",
    "        k_data_num = undesired_index[i][0]\n",
    "        k_l = undesired_index[i][1]\n",
    "\n",
    "        undesired_data = tf.reshape(network.x[k_data_num,:], [1,-1])\n",
    "\n",
    "        ## Remove the data that does not meet the error term\n",
    "        left_data = network.x[:k_data_num,:]\n",
    "        right_data = network.x[k_data_num+1:,:]\n",
    "        remain_tensor = tf.concat([left_data, right_data], 0)\n",
    "\n",
    "        \n",
    "        ## Use the random method to find out the gamma and zeta\n",
    "        while True:\n",
    "\n",
    "            ## Find m-vector gamma: r\n",
    "            ## Use the random method to generate the gamma that can make the conditions met\n",
    "            gamma = tf.random.uniform(shape=[1,network.x.shape[1]])\n",
    "            subtract_undesired_data = tf.subtract(remain_tensor, undesired_data)\n",
    "            matmul_value = tf.matmul(gamma,tf.transpose(subtract_undesired_data))\n",
    "\n",
    "\n",
    "            if tf.reduce_all(matmul_value != 0):\n",
    "\n",
    "                while True:\n",
    "\n",
    "                    ## Find the tiny value: zeta\n",
    "                    ## Use the random method to generate the zeta that can make the conditions met\n",
    "                    zeta = tf.random.uniform(shape=[1])\n",
    "\n",
    "                    if tf.reduce_all(tf.multiply(tf.add(zeta,matmul_value),tf.subtract(zeta,matmul_value))<0):\n",
    "                        break\n",
    "\n",
    "                break\n",
    "\n",
    "\n",
    "        ## The weight of input layer to hidden layer I\n",
    "        w10 = gamma\n",
    "        w11 = gamma\n",
    "        w12 = gamma\n",
    "        \n",
    "        W1_new = tf.transpose(tf.concat([w10,w11,w12],0))\n",
    "\n",
    "        ## The bias of input layer to hidden layer I\n",
    "        matual_value = tf.matmul(gamma,tf.transpose(undesired_data))\n",
    "\n",
    "        b10 = tf.subtract(zeta,matual_value)\n",
    "        b11 = -1*matual_value\n",
    "        b12 = tf.subtract(-1*zeta,matual_value)\n",
    "        b1_new = tf.reshape(tf.concat([b10,b11,b12],0),[3])\n",
    "\n",
    "        ## The weight of hidden layer I to output layer\n",
    "        gap = network.y[k_data_num, k_l]-yo[k_data_num, k_l]\n",
    "\n",
    "        wo0_value = gap/zeta\n",
    "        wo1_value = (-2*gap)/zeta\n",
    "        wo2_value = gap/zeta\n",
    "\n",
    "        wo0 = tf.reshape(tf.one_hot(k_l,4,dtype=tf.float32) * wo0_value, shape=(1,-1))\n",
    "        wo1 = tf.reshape(tf.one_hot(k_l,4,dtype=tf.float32) * wo1_value, shape=(1,-1))\n",
    "        wo2 = tf.reshape(tf.one_hot(k_l,4,dtype=tf.float32) * wo2_value, shape=(1,-1))\n",
    "\n",
    "        Wo_new = tf.concat([wo0,wo1,wo2],0)\n",
    "\n",
    "        ## Add new neuroes to the network\n",
    "        network.W1 = tf.Variable(tf.concat([network.W1, W1_new],1), tf.float32)\n",
    "        network.b1 = tf.Variable(tf.concat([network.b1, b1_new],0), tf.float32)\n",
    "        network.Wo = tf.Variable(tf.concat([network.Wo, Wo_new],0), tf.float32)\n",
    "\n",
    "        yo, loss, tape = network.forward()\n",
    "   \n",
    "        ## Determine if cramming is successful and print out the corresponding information\n",
    "        if tf.reduce_all(tf.math.abs(yo[k_data_num,k_l]-network.y[k_data_num,k_l]) <= network.threshold_for_error):\n",
    "            \n",
    "            ## If the cramming process is complete, set the acceptable of the network as true\n",
    "            if i==(undesired_index.shape[0]-1):\n",
    "                network.acceptable = True\n",
    "            \n",
    "            print(\"Cramming success!\")\n",
    "            \n",
    "        \n",
    "        else:\n",
    "            print(\"Cramming failed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularizing module (Check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regularizing(network):\n",
    "\n",
    "    ## Record the number of executions\n",
    "    times_enlarge = 0\n",
    "    times_shrink = 0\n",
    "    ## Set up the learning rate of the network\n",
    "    network.learning_rate = 1e-3\n",
    "\n",
    "    ## Set epoch to 100\n",
    "    for i in range(100):\n",
    "\n",
    "        ## Store the parameter of the network\n",
    "        network_pre = copy.deepcopy(network)\n",
    "        yo, loss, tape = network.forward(1e-2)\n",
    "        loss_pre = loss\n",
    "\n",
    "        ## Backward operation to obtain w'\n",
    "        network.backward_Adam(tape, loss)\n",
    "        yo, loss, tape = network.forward(1e-2)\n",
    "\n",
    "         # Confirm whether the adjusted loss value is smaller than the current one\n",
    "        if loss <= loss_pre:\n",
    "            \n",
    "            ## Identify that all forecast value has met the error term\n",
    "            if tf.reduce_all(tf.math.abs(yo-network.y) <= network.threshold_for_error):\n",
    "                \n",
    "                ## If true, multiply the learning rate by 1.2\n",
    "                network.learning_rate *= 1.2\n",
    "                times_enlarge += 1\n",
    "#                 print(\"Regularizing process\")\n",
    "\n",
    "            else:\n",
    "\n",
    "                ## Else, restore w and end the process\n",
    "                network = network_pre\n",
    "                print(\"Number of enlarge:\",times_enlarge)\n",
    "                print(\"Number of shrink:\",times_shrink)\n",
    "                print(\"Regularizing result: Unable to meet the error term\")\n",
    "                return(network)\n",
    "\n",
    "        # If the adjusted loss value is not smaller than the current one\n",
    "        else:\n",
    "           \n",
    "\n",
    "            ## If the learning rate is greater than the threshold for learning rate\n",
    "            if network.learning_rate > network.threshold_for_lr:\n",
    "                \n",
    "                ## Restore the w and multiply the learning rate by 0.7\n",
    "                network = network_pre\n",
    "                network.learning_rate *= 0.7\n",
    "                times_shrink += 1\n",
    "             ## If the learning rate is smaller than the threshold for learning rate\n",
    "            else:\n",
    "                \n",
    "                ## Restore the w\n",
    "                network = network_pre\n",
    "                print(\"Number of enlarge:\",times_enlarge)\n",
    "                print(\"Number of shrink:\",times_shrink)\n",
    "                print(\"Regularizing result: Less than the epsilon for the learning rate\")\n",
    "                return(network)\n",
    "\n",
    "        if i == 99:\n",
    "            print(\"Number of enlarge:\",times_enlarge)\n",
    "            print(\"Number of shrink:\",times_shrink)\n",
    "            print(\"Regularizing result: The number of rounds has reached\")\n",
    "            return(network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reorganizing module (Check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reorganizing(network):\n",
    "    \n",
    "    ## Set up the k = 1, and p = the number of hidden node\n",
    "    k = 1\n",
    "    p = network.W1.shape[1]\n",
    "\n",
    "    while True:\n",
    "\n",
    "        ## If k > p, end of Process\n",
    "        if k > p:\n",
    "\n",
    "            print(\"Reorganizing result: The final number of neuro is \",p)\n",
    "            return(network)\n",
    "\n",
    "        ## Else, Process is ongoing\n",
    "        else:\n",
    "\n",
    "            ## Using the regularizing module to adjust the network\n",
    "            network = regularizing(network)\n",
    "            \n",
    "            ## Store the network and w\n",
    "            network_pre = copy.deepcopy(network)\n",
    "\n",
    "            ## Set up the acceptable of the network as false\n",
    "            network.acceptable = False\n",
    "\n",
    "            ## Ignore the K hidden node\n",
    "            network.W1 = tf.Variable(tf.concat([network.W1[:,:k-1],network.W1[:,k:]],1))\n",
    "            network.b1 = tf.Variable(tf.concat([network.b1[:k-1],network.b1[k:]],0))\n",
    "            network.Wo = tf.Variable(tf.concat([network.Wo[:k-1,:],network.Wo[k:,:]],0))\n",
    "\n",
    "            ## Using the matching module to adjust the network\n",
    "            network = matching(network)\n",
    "\n",
    "            ## If the resulting network is acceptable, this means that the k hidden node can be removed\n",
    "            if network.acceptable:\n",
    "\n",
    "                print(\"Drop out the nero number: %d / %d\" %(k, p))\n",
    "                network.nb_node_pruned += 1\n",
    "                ## p--\n",
    "                p-=1\n",
    "\n",
    "            ## Else, it means that the k hidden node cannot be removed\n",
    "            else:\n",
    "                \n",
    "                ## Restore the network and w\n",
    "                network = network_pre\n",
    "                print(\"Cannot drop out the nero number: %d / %d\" %(k, p))\n",
    "                \n",
    "                ## k++\n",
    "                k+=1\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Call the help method \"get_data(...)\" to get the training data and test data \n",
    "x_train_data, x_test_data, y_train_data, y_test_data = get_data(4)\n",
    "\n",
    "## Use min-max normalization to normalize data in a range of 0 to 1\n",
    "x_train_scaled = sc.fit_transform(x_train_data)\n",
    "x_test_scaled = sc.transform(x_test_data)\n",
    "y_train_scaled = sc.fit_transform(y_train_data)\n",
    "\n",
    "## Use the first 25 training data to debug\n",
    "x_train_scaled = x_train_scaled[:22]\n",
    "y_train_scaled = y_train_scaled[:22]\n",
    "\n",
    "## Pick up m+1 data that are linearly independent as the initial m+1 training data \n",
    "initial_x = x_train_scaled[:x_train_scaled.shape[1]+1]\n",
    "initial_y = y_train_scaled[:x_train_scaled.shape[1]+1]\n",
    "\n",
    "## Construct the network object with 4 neuros\n",
    "network = Network(4, initial_x, initial_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network.b1.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct a instance of network\n",
    "- trained through the matching module, reorganizing module, and cramming module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data index: 20\n",
      "The loss value of k: (0.041099656, 0)\n",
      "Selecting module finish!\n",
      "The error term for each data\n",
      "tf.Tensor(\n",
      "[[0.00000203 0.00000042 0.00000083 0.        ]\n",
      " [0.00000149 0.00000107 0.00000012 0.00000066]\n",
      " [0.00000572 0.00000775 0.00000006 0.        ]\n",
      " [0.         0.00000328 0.00000012 0.0000003 ]\n",
      " [0.00000542 0.00000322 0.00000054 0.00000024]\n",
      " [0.00000811 0.00000292 0.00000036 0.00000036]\n",
      " [0.00000399 0.00000656 0.00000024 0.00000024]\n",
      " [0.00000787 0.00000739 0.00000036 0.0000003 ]\n",
      " [0.00000703 0.00000107 0.00000018 0.00000036]\n",
      " [0.00000358 0.00000256 0.00000024 0.00000018]\n",
      " [0.0000059  0.00000072 0.0000003  0.00000048]\n",
      " [0.00000203 0.00000709 0.00000036 0.00000018]\n",
      " [0.0000003  0.00000727 0.00000077 0.0000003 ]\n",
      " [0.00000143 0.00000101 0.00000066 0.00000036]\n",
      " [0.00000554 0.00000411 0.00000048 0.        ]\n",
      " [0.00000459 0.00000095 0.00000036 0.00000089]\n",
      " [0.00000393 0.00000775 0.00000089 0.00000012]\n",
      " [0.00000477 0.00000042 0.00000048 0.00000036]\n",
      " [0.00000614 0.00000381 0.00000119 0.0000003 ]\n",
      " [0.37636077 0.08499801 0.11014491 0.05826908]], shape=(20, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "nb_step4 = 0\n",
    "nb_step6_1 = 0\n",
    "nb_step6_2 = 0\n",
    "\n",
    "## Call the help method \"get_data(...)\" to get the training data and test data \n",
    "x_train_data, x_test_data, y_train_data, y_test_data = get_data(4)\n",
    "\n",
    "## Use min-max normalization to normalize data in a range of 0 to 1\n",
    "x_train_scaled = sc.fit_transform(x_train_data)\n",
    "x_test_scaled = sc.transform(x_test_data)\n",
    "y_train_scaled = sc.fit_transform(y_train_data)\n",
    "\n",
    "## Use the first 25 training data to debug\n",
    "# x_train_scaled = x_train_scaled[:22]\n",
    "# y_train_scaled = y_train_scaled[:22]\n",
    "\n",
    "## Pick up m+1 data that are linearly independent as the initial m+1 training data \n",
    "initial_x = x_train_scaled[:x_train_scaled.shape[1]+1]\n",
    "initial_y = y_train_scaled[:x_train_scaled.shape[1]+1]\n",
    "\n",
    "## The remaining data (that exclude initial data)\n",
    "x_train_scaled = x_train_scaled[x_train_scaled.shape[1]+1:]\n",
    "y_train_scaled = y_train_scaled[x_train_scaled.shape[1]+1:]\n",
    "\n",
    "## Construct the network object with 4 neuros\n",
    "network = Network(4, initial_x, initial_y)\n",
    "\n",
    "## Use initilize module to set up the initial network\n",
    "initializing(network, initial_x, initial_y)\n",
    "\n",
    "\n",
    "## Training of all data\n",
    "for i in range(0, x_train_scaled.shape[0]):\n",
    "    \n",
    "   \n",
    "    ## Print out some info for debug\n",
    "    print(\"The data index: %d\"%(i+x_train_scaled.shape[1]+2))\n",
    "    \n",
    "    ## Get the data index by selecting module to sort the data by the loss value from smallest to largest\n",
    "    sorted_index = selecting(network, x_train_scaled, y_train_scaled)\n",
    "    \n",
    "    ## Add new data for training\n",
    "    network.addData(x_train_scaled[sorted_index[0]], y_train_scaled[sorted_index[0]])\n",
    "    x_train_scaled = np.delete(x_train_scaled, sorted_index[0], 0)\n",
    "    y_train_scaled = np.delete(y_train_scaled, sorted_index[0], 0)\n",
    "    \n",
    "    yo, loss, tape = network.forward()\n",
    "    \n",
    "    ## Print out some information for debug\n",
    "    print(\"The error term for each data\")\n",
    "    print(tf.math.abs(yo-network.y))\n",
    "    \n",
    "    ## Determine whether the forecast value can meet the error term\n",
    "    if tf.reduce_all(tf.math.abs(yo-network.y) <= network.threshold_for_error):\n",
    "        \n",
    "        ## If true, set up the acceptable of the network as true\n",
    "        network.acceptable = True\n",
    "        \n",
    "        ## Use reorganizing module to adjust the model\n",
    "        network = reorganizing(network)\n",
    "        \n",
    "        ## Record the number of runs\n",
    "        nb_step4 += 1\n",
    " \n",
    "    else:\n",
    "        \n",
    "        ## If true, set up the acceptable of the network as false\n",
    "        network.acceptable = False\n",
    "        network_pre = copy.deepcopy(network)\n",
    "        \n",
    "        ## Use matching module to adjust the model\n",
    "        network = matching(network)\n",
    "        \n",
    "        ## If the output of the matching module is an acceptable network, use the reorganization module to adjust the model\n",
    "        if network.acceptable:\n",
    "            network = reorganizing(network)\n",
    "            \n",
    "            ## Record the number of runs\n",
    "            nb_step6_1 += 1\n",
    " \n",
    "        ## Else (if the output of the matching module is an unacceptable network)\n",
    "        else:\n",
    "            network = network_pre\n",
    "            \n",
    "            ## Use cramming module and reorganizing module to adjust the model\n",
    "            cramming(network)\n",
    "            network = reorganizing(network)\n",
    "            \n",
    "            ## Record the number of runs\n",
    "            nb_step6_2 += 1\n",
    "\n",
    "    ## Print out the model status\n",
    "    network.nb_node_acceptable = tf.concat([network.nb_node_acceptable, [network.b1.shape[0]]],0)\n",
    "    print(\"The network status:\",network.acceptable)\n",
    "    print(\"-\"*10,\"next data\",\"-\"*10)\n",
    "## Calculate the training time    \n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_value = network.forecast(x_test_scaled)\n",
    "\n",
    "y_pred = sc.inverse_transform(forecast_value)\n",
    "accuracy = list()\n",
    "\n",
    "for i in range(y_pred.shape[1]):\n",
    "#     for _ in range(y_pred.shape[0]): \n",
    "\n",
    "    correct_times = np.sum(tf.math.abs(y_test_data[:,i]-y_pred[:,i])<2000)\n",
    "    accuracy.append(correct_times/y_pred.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The training time(s):\",end - start)\n",
    "\n",
    "print(\"-\"*30)\n",
    "total_time = nb_step4 + nb_step6_1 + nb_step6_2\n",
    "print(\"The percentage of each step\")\n",
    "print(\"Step 4:%.2f\"%(nb_step4/total_time))\n",
    "print(\"Step 6.1:%.2f\"%(nb_step6_1/total_time))\n",
    "print(\"Step 6.2:%.2f\"%(nb_step6_2/total_time))\n",
    "\n",
    "print(\"-\"*30)\n",
    "print(\"Total amount of cramming occurrences:\",nb_step6_2)\n",
    "\n",
    "print(\"-\"*30)\n",
    "print(\"The amount of hidden node that be pruned:\",network.nb_node_pruned)\n",
    "\n",
    "print(\"-\"*30)\n",
    "print(\"The amount of adopted hidden nodes:\",network.nb_node_acceptable[-1].numpy())\n",
    "\n",
    "print(\"-\"*30)\n",
    "print(\"Accuracy\")\n",
    "print(\"The accuracy for the t+1:\",accuracy[0])\n",
    "print(\"The accuracy for the t+2:\",accuracy[1])\n",
    "print(\"The accuracy for the t+3:\",accuracy[2])\n",
    "print(\"The accuracy for the t+4:\",accuracy[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yo, loss, tape = network.forward()\n",
    "plt.plot(sc.inverse_transform(yo)[:,0], label=\"LLAAT\")\n",
    "plt.plot(sc.inverse_transform(network.y)[:,0], label=\"Actual\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forecast_value = network.forecast(x_test_scaled)\n",
    "plt.plot(sc.inverse_transform(forecast_value)[:,0], label=\"LLAAT\")\n",
    "plt.plot(y_test_data[:,0], label=\"Actual\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
