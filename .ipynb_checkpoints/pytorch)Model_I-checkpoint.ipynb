{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import related package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import tensorflow package for modeling\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "\n",
    "## Data processing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "## Min-max normalization\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "## Plot the graph\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "## Initializing module\n",
    "from sklearn.linear_model import LinearRegression\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "## Copy module\n",
    "import copy\n",
    "\n",
    "## Used to calculate the training time\n",
    "import time\n",
    "\n",
    "## Set the GUP environment\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set up the display\n",
    "torch.set_printoptions(sci_mode=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 編寫結果在note中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "# Setup logging.\n",
    "logging.basicConfig(\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    datefmt='%m/%d/%Y %I:%M:%S %p',\n",
    "    level=logging.DEBUG,\n",
    "    filename='log.txt'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Control memory usage space for GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "目前設備： 0\n",
      "目前設備名： GeForce GTX 1070 Ti\n"
     ]
    }
   ],
   "source": [
    "## 查詢有無可用 GPU\n",
    "torch.cuda.is_available()\n",
    "## 查詢可用 GPU 的數量\n",
    "torch.cuda.device_count()\n",
    "##目前設備\n",
    "print(\"目前設備：\",torch.cuda.current_device())\n",
    "## 目前設備名\n",
    "print(\"目前設備名：\",torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpus = tf.config.list_physical_devices(device_type='GPU')\n",
    "\n",
    "# tf.config.set_logical_device_configuration(\n",
    "#     gpus[0],\n",
    "#     [tf.config.LogicalDeviceConfiguration(memory_limit=2048),\n",
    "#      tf.config.LogicalDeviceConfiguration(memory_limit=2048)]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ['CUDA_VISIBLE_DEVICES'] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config = tf.compat.v1.ConfigProto()\n",
    "# config.gpu_options.per_process_gpu_memory_fraction =0.1\n",
    "# tf.compat.v1.Session(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpu_options = tf.compat.v1.GPUOptions(per_process_gpu_memory_fraction=0.5)\n",
    "# sess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(gpu_options=gpu_options))\n",
    "# tf.compat.v1.keras.backend.set_session(sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print out some info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_cacl(pred_value, actual_value):\n",
    "    \n",
    "#     yo, loss, tape = network.forward()\n",
    "    accuracy = []\n",
    "\n",
    "    for i in range(pred_value.shape[1]):\n",
    "        \n",
    "        correct_times = torch.nonzero(torch.abs(pred_value[:,i] - actual_value[:,i]) < 2000)\n",
    "        accuracy.append(correct_times.shape[0]/pred_value.shape[0])   \n",
    "        \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_result(name, pred_value, actual_value):\n",
    "    \n",
    "    fig, ax = plt.subplots(2,2,figsize=(20,10), sharex=True, sharey=True)\n",
    "\n",
    "    for i in range(yo.shape[1]):\n",
    "        ax[i//2,i%2].plot(pred_value[:,i], label=\"LLAAT\")\n",
    "        ax[i//2,i%2].plot(actual_value[:,i], label=\"Actual\")\n",
    "        ax[i//2,i%2].set_title(\"Forecasted performance for l=%d\" %(i+1))\n",
    "        ax[i//2,i%2].legend()\n",
    "    #fig.text(0.5, 0, \"Stage of training\", ha='center', fontsize=20)\n",
    "    #fig.text(0, 0.5, \"Copper price value\", va='center', rotation='vertical')\n",
    "    fig.suptitle(\"In the %s process\"%(name))\n",
    "    fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_adopted_node(network):\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(20,5))\n",
    "    ax.set_title(\"Total amount of adopted hidden nodes in the training process\")\n",
    "    ax.plot(network.nb_node_acceptable,\"-o\")\n",
    "\n",
    "    ax.set_xlabel(\"Stage of training\")\n",
    "    ax.set_ylabel(\"Hidden nodes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(network, nb_step4, nb_step6_1, nb_step6_2, x_test, y_test, start, end):\n",
    "\n",
    "    ## Training_Step\n",
    "    \n",
    "    print(\"<<Training step>>\")\n",
    "    print(\"The training time(s):\",end - start)\n",
    "    yo, loss= network.forward()\n",
    "    accuracy_train = accuracy_cacl(yo.data, network.y.data)\n",
    "\n",
    "    ## Test_step\n",
    "#     print(\"<<Testing step>>\")\n",
    "    pred_value_test = network.forecast(x_test).data\n",
    "    accuracy_test = accuracy_cacl(pred_value_test.data, y_test)\n",
    "    \n",
    "    total_time = nb_step4 + nb_step6_1 + nb_step6_2\n",
    "    print(\"<<The percentage of each step>>\")\n",
    "    print(\"Step 4: %.2f%%\"%(nb_step4/total_time))\n",
    "    print(\"Step 6.1: %.2f%%\"%(nb_step6_1/total_time))\n",
    "    print(\"Step 6.2: %.2f%%\"%(nb_step6_2/total_time))\n",
    "\n",
    "    print(\"-\"*60)\n",
    "    print(\"Total frequency of cramming occurrences:\",nb_step6_2)\n",
    "\n",
    "    print(\"-\"*60)\n",
    "    print(\"The amount of hidden node that be pruned:\",network.nb_node_pruned)\n",
    "\n",
    "    print(\"-\"*60)\n",
    "    print(\"The amount of adopted hidden nodes:\",network.nb_node_acceptable[-1].item())\n",
    "\n",
    "    print(\"-\"*60)\n",
    "    print(\"<<Accuracy in training step>>\")\n",
    "    print(\"The accuracy for l = 1: %.1f%%\" %(accuracy_train[0]*100))\n",
    "    print(\"The accuracy for l = 2: %.1f%%\" %(accuracy_train[1]*100))\n",
    "    print(\"The accuracy for l = 3: %.1f%%\" %(accuracy_train[2]*100))\n",
    "    print(\"The accuracy for l = 4: %.1f%%\" %(accuracy_train[3]*100))\n",
    "\n",
    "\n",
    "    print(\"-\"*60)\n",
    "    print(\"<<Accuracy in inferencing step>>\")\n",
    "    print(\"The accuracy for l = 1: %.1f%%\" %(accuracy_test[0]*100))\n",
    "    print(\"The accuracy for l = 2: %.1f%%\" %(accuracy_test[1]*100))\n",
    "    print(\"The accuracy for l = 3: %.1f%%\" %(accuracy_test[2]*100))\n",
    "    print(\"The accuracy for l = 4: %.1f%%\" %(accuracy_test[3]*100))\n",
    "\n",
    "\n",
    "    plot_result(\"training\",yo.data, network.y.data)\n",
    "    plot_result(\"inferencing\",pred_value_test, y_test_data)\n",
    "    plot_adopted_node(network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read(path):\n",
    "    return pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildTrain(train, pastWeek=4, futureWeek=4, defaultWeek=1):\n",
    "    X_train, Y_train = [], []\n",
    "    for i in range(train.shape[0]-futureWeek-pastWeek):\n",
    "        X = np.array(train.iloc[i:i+defaultWeek])\n",
    "        X = np.append(X,train[\"CCSP\"].iloc[i+defaultWeek:i+pastWeek])\n",
    "        X_train.append(X.reshape(X.size))\n",
    "        Y_train.append(np.array(train.iloc[i+pastWeek:i+pastWeek+futureWeek][\"CCSP\"]))\n",
    "    return np.array(X_train), np.array(Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Min-max normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Use min-max normalization to scale the data to the range from 1 to 0\n",
    "sc = MinMaxScaler(feature_range = (0, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Design get_data() to get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(futureWeek):\n",
    "    \n",
    "    ## Read weekly copper price data\n",
    "    path = \"WeeklyFinalData.csv\"\n",
    "    data = read(path)\n",
    "    \n",
    "    date = data[\"Date\"]\n",
    "    data.drop(\"Date\", axis=1, inplace=True)\n",
    "    \n",
    "    ## Add time lag (pastWeek=4, futureWeek=1)\n",
    "    x_data, y_data = buildTrain(data, futureWeek=futureWeek)\n",
    "\n",
    "    \n",
    "    ## Split the data to training data and test data\n",
    "    x_train_data = x_data[:int(x_data.shape[0]*0.8)]\n",
    "    x_test_data = x_data[int(x_data.shape[0]*0.8):]\n",
    "    y_train_data = y_data[:int(x_data.shape[0]*0.8)]\n",
    "    y_test_data = y_data[int(x_data.shape[0]*0.8):]\n",
    "\n",
    "\n",
    "    return (x_train_data, x_test_data, y_train_data, y_test_data)\n",
    "\n",
    "#     return (x_data, y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(376, 18)\n",
      "(95, 18)\n",
      "(376, 4)\n",
      "(95, 4)\n"
     ]
    }
   ],
   "source": [
    "x_train_data, x_test_data, y_train_data, y_test_data = get_data(4)\n",
    "print(x_train_data.shape)\n",
    "print(x_test_data.shape)\n",
    "print(y_train_data.shape)\n",
    "print(y_test_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing module (Check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initializing(network, initial_x, initial_y):\n",
    "    print(\"Initializing module\")\n",
    "    ## Find each minimum output value y\n",
    "    min_y = torch.min(initial_y, axis=0)\n",
    "\n",
    "    ## Subtract min_y from each y\n",
    "    res_y = initial_y-min_y.values\n",
    "    \n",
    "    ## Use linear regression to find the initial W1,b1,Wo,bo\n",
    "    reg = LinearRegression().fit(initial_x, res_y)\n",
    "#     ## Set up the initial parameter of the network\n",
    "    network.linear1.weight = torch.nn.Parameter(torch.FloatTensor(reg.coef_).cuda())\n",
    "#     network.linear1.weight = network.linear1.weight.cuda()\n",
    "    network.linear1.bias = torch.nn.Parameter(torch.FloatTensor(reg.intercept_).cuda())\n",
    "    network.linear2.weight=torch.nn.Parameter(torch.FloatTensor([[1,0,0,0], [0,1,0,0],[0,0,1,0],[0,0,0,1]]).cuda())\n",
    "    network.linear2.bias = torch.nn.Parameter(torch.FloatTensor(min_y.values).cuda())\n",
    "    ## Set up the acceptable of the initial network as True\n",
    "    network.acceptable =True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting module (Check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def selecting(network, x_train_scaled, y_train_scaled):\n",
    "    \n",
    "    print(\"<<Selecting module>>\")\n",
    "    loss = []\n",
    "    temp_network = copy.deepcopy(network)\n",
    "    \n",
    "    ## Put each data into network to calculate the loss value\n",
    "    for i in range(x_train_scaled.shape[0]):\n",
    "        temp_network.setData(x_train_scaled[i].reshape(1,-1), y_train_scaled[i].reshape(1,-1))\n",
    "        loss.append((temp_network.forward()[1].item(),i))\n",
    "\n",
    "#     ## Sort the data according to the loss value from smallest to largest, and save the data index in sorted_index\n",
    "    sorted_index = [sorted_data[1] for sorted_data in sorted(loss, key = lambda x:x[0])]\n",
    "    \n",
    "    \n",
    "    ## Print out some info for debug\n",
    "    print(\"The loss value of k:\",loss[sorted_index[0]])\n",
    "#     print(\"The second_loss value of k:\",loss[sorted_index[1]])\n",
    "    print(\"Selecting module finish!\")\n",
    "    \n",
    "    return sorted_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matching(network):\n",
    "\n",
    "    times_enlarge=0\n",
    "    times_shrink=0\n",
    "    print(\"<<Matching module>>\")\n",
    "    ## Set up the learning rate of the network\n",
    "    network.learning_rate = 1e-3\n",
    "#     yo, loss = network.forward()\n",
    "    network.acceptable = False\n",
    "    initial_network = copy.deepcopy(network)\n",
    "#     while True:\n",
    "        \n",
    "#         print(\"最後審判\")\n",
    "#         print(torch.abs(yo-network.y))\n",
    "#         print(\"你的Loss值\")\n",
    "#         print(loss)\n",
    "#         if torch.all(torch.abs(yo-network.y) <= network.threshold_for_error):\n",
    "\n",
    "#             ## If true, set the acceptable of the network as true and return it\n",
    "#             network.acceptable = True\n",
    "#             print(\"Matching finished - the network is acceptable\")\n",
    "#             print(\"<<Final Network>>\")\n",
    "#             print(network.state_dict())\n",
    "# #             print(\"<<Pre-network>>\")\n",
    "# #             print(network_pre.state_dict())\n",
    "#             return(network)\n",
    "    \n",
    "    \n",
    "    while True:\n",
    "\n",
    "        yo, loss = network.forward()\n",
    "        network_pre = copy.deepcopy(network)\n",
    "        loss_pre = loss\n",
    "\n",
    "#         print(\"<<再次確認一下要調整的network\")\n",
    "#             print(network.state_dict())\n",
    "#         print(torch.abs(yo-network.y))\n",
    "#         print(\"你的Loss值\",loss)\n",
    "#         print(\"loss2\", loss_pre)\n",
    "#         print(\"<<Before>>\")\n",
    "#             print(\"lr\",network.learning_rate)\n",
    "#         print(\"W1\",network.linear1.weight)\n",
    "\n",
    "            # Backward and check the loss performance of the network with new learning rate\n",
    "        network.backward_Adam(loss)\n",
    "        yo, loss = network.forward()\n",
    "\n",
    "#         print(\"調整後看一下\")\n",
    "#         print(torch.abs(yo-network.y))\n",
    "#         print(\"我看一下Loss值\",loss)\n",
    "#             print(\"<<更新後的network\")\n",
    "#             print(network.state_dict())\n",
    "#         print(\"los3\", loss)\n",
    "\n",
    "#         print(\"<<After>>\")\n",
    "#             print(\"lr\",network.learning_rate)\n",
    "#         print(\"W1\",network.linear1.weight)\n",
    "              ## Identify that all forecast value has met the error term\n",
    "\n",
    "            # Confirm whether the loss value of the adjusted network is smaller than the current one\n",
    "        if loss < loss_pre and torch.all(torch.abs(yo-network.y) <= network.threshold_for_error):\n",
    "            \n",
    "#             print(\"我成功了\")\n",
    "            # If true, multiply the learning rate by 1.2\n",
    "            network.acceptable = True\n",
    "            print(\"Matching finished - the network is acceptable\")\n",
    "#             print(\"<<Final Network>>\")\n",
    "#             print(network.state_dict())\n",
    "#             print(\"<<Pre-network>>\")\n",
    "#             print(network_pre.state_dict())\n",
    "            print(\"Number of enlarge:\",times_enlarge)\n",
    "            print(\"Number of shrink:\",times_shrink)\n",
    "            return(network)\n",
    "\n",
    "\n",
    "#                 print(\"<<Enlarge>>\")\n",
    "        # On the contrary, reduce the learning rate\n",
    "        elif loss < loss_pre:\n",
    "#             print(\"快成功了，加油\")\n",
    "            times_enlarge+=1\n",
    "            network.learning_rate *= 1.2\n",
    "            \n",
    "    \n",
    "        else:         \n",
    "\n",
    "            # Identify whether the current learning rate is less than the threshold\n",
    "            if network.learning_rate <= network.threshold_for_lr:\n",
    "\n",
    "                # If true, set the acceptable of the network as false and return it\n",
    "                network.acceptable = False\n",
    "                print(\"Matching finished - the network is Unacceptable\")\n",
    "                print(\"Number of enlarge:\",times_enlarge)\n",
    "                print(\"Number of shrink:\",times_shrink)\n",
    "                return(initial_network)\n",
    "\n",
    "            # On the contrary, restore w and adjust the learning rate\n",
    "            else:\n",
    "#                 print(\"我在縮小\")\n",
    "                # Restore the papameter of the network\n",
    "                network = copy.deepcopy(network_pre)\n",
    "                times_shrink+=1\n",
    "                network.learning_rate *= 0.7\n",
    "\n",
    "#                     print(\"<<After>>\")\n",
    "#                     print(network.learning_rate)\n",
    "#                     print(\"<<Shrink>>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matching for reorganizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def matching_for_reorganizing(network):\n",
    "\n",
    "#     times_enlarge=0\n",
    "#     times_shrink=0\n",
    "#     print(\"<<Matching module>>\")\n",
    "#     ## Set up the learning rate of the network\n",
    "#     network.learning_rate = 1e-3\n",
    "# #     yo, loss = network.forward()\n",
    "#     network.acceptable = False\n",
    "#     initial_network = copy.deepcopy(network)\n",
    "# #     while True:\n",
    "        \n",
    "# #         print(\"最後審判\")\n",
    "# #         print(torch.abs(yo-network.y))\n",
    "# #         print(\"你的Loss值\")\n",
    "# #         print(loss)\n",
    "# #         if torch.all(torch.abs(yo-network.y) <= network.threshold_for_error):\n",
    "\n",
    "# #             ## If true, set the acceptable of the network as true and return it\n",
    "# #             network.acceptable = True\n",
    "# #             print(\"Matching finished - the network is acceptable\")\n",
    "# #             print(\"<<Final Network>>\")\n",
    "# #             print(network.state_dict())\n",
    "# # #             print(\"<<Pre-network>>\")\n",
    "# # #             print(network_pre.state_dict())\n",
    "# #             return(network)\n",
    "    \n",
    "    \n",
    "#     for i in range(100):\n",
    "\n",
    "#         yo, loss = network.forward()\n",
    "#         network_pre = copy.deepcopy(network)\n",
    "#         loss_pre = loss\n",
    "\n",
    "# #         print(\"<<再次確認一下要調整的network\")\n",
    "# #             print(network.state_dict())\n",
    "# #         print(\"誤差值\")\n",
    "# #         print(torch.abs(yo-network.y))\n",
    "# #         print(\"你的Loss值\",loss)\n",
    "# #         print(\"loss2\", loss_pre)\n",
    "# #         print(\"<<Before>>\")\n",
    "# #             print(\"lr\",network.learning_rate)\n",
    "# #         print(\"W1\",network.linear1.weight)\n",
    "\n",
    "#             # Backward and check the loss performance of the network with new learning rate\n",
    "#         network.backward_Adam(loss)\n",
    "#         yo, loss = network.forward()\n",
    "\n",
    "# #         print(\"調整後看一下\")\n",
    "# #         print(\"誤差值\")\n",
    "# #         print(torch.abs(yo-network.y))\n",
    "# #         print(\"我看一下Loss值\",loss)\n",
    "# #             print(\"<<更新後的network\")\n",
    "# #             print(network.state_dict())\n",
    "# #         print(\"loss\", loss)\n",
    "\n",
    "# #         print(\"<<After>>\")\n",
    "# #             print(\"lr\",network.learning_rate)\n",
    "# #         print(\"W1\",network.linear1.weight)\n",
    "#               ## Identify that all forecast value has met the error term\n",
    "\n",
    "#             # Confirm whether the loss value of the adjusted network is smaller than the current one\n",
    "#         if loss < loss_pre and torch.all(torch.abs(yo-network.y) <= network.threshold_for_error):\n",
    "            \n",
    "#             print(\"我成功了\")\n",
    "#             # If true, multiply the learning rate by 1.2\n",
    "#             network.acceptable = True\n",
    "#             print(\"Matching finished - the network is acceptable\")\n",
    "# #             print(\"<<Final Network>>\")\n",
    "# #             print(network.state_dict())\n",
    "# #             print(\"<<Pre-network>>\")\n",
    "# #             print(network_pre.state_dict())\n",
    "#             print(\"Number of enlarge:\",times_enlarge)\n",
    "#             print(\"Number of shrink:\",times_shrink)\n",
    "#             return(network)\n",
    "\n",
    "\n",
    "# #                 print(\"<<Enlarge>>\")\n",
    "#         # On the contrary, reduce the learning rate\n",
    "#         elif loss < loss_pre:\n",
    "#             #print(\"快成功了，加油\")\n",
    "#             times_enlarge+=1\n",
    "#             network.learning_rate *= 1.2\n",
    "            \n",
    "    \n",
    "#         else:         \n",
    "\n",
    "#             # Identify whether the current learning rate is less than the threshold\n",
    "#             if network.learning_rate <= network.threshold_for_lr:\n",
    "\n",
    "#                 # If true, set the acceptable of the network as false and return it\n",
    "#                 network.acceptable = False\n",
    "#                 print(\"Matching finished - the network is Unacceptable\")\n",
    "#                 print(\"Number of enlarge:\",times_enlarge)\n",
    "#                 print(\"Number of shrink:\",times_shrink)\n",
    "#                 return(initial_network)\n",
    "\n",
    "#             # On the contrary, restore w and adjust the learning rate\n",
    "#             else:\n",
    "#                 #print(\"我在縮小\")\n",
    "#                 # Restore the papameter of the network\n",
    "#                 network = copy.deepcopy(network_pre)\n",
    "#                 times_shrink+=1\n",
    "#                 network.learning_rate *= 0.7\n",
    "\n",
    "                \n",
    "#     network.acceptable = False\n",
    "#     print(\"Matching的第%d回合\"%(i+1))\n",
    "#     print(\"Matching finished - the network is Unacceptable\")\n",
    "#     print(\"Number of enlarge:\",times_enlarge)\n",
    "#     print(\"Number of shrink:\",times_shrink)\n",
    "#     return(network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Record program start time\n",
    "# start = time.time()\n",
    "\n",
    "# ## Record the number of each step\n",
    "# nb_step4 = 0\n",
    "# nb_step6_1 = 0\n",
    "# nb_step6_2 = 0\n",
    "\n",
    "# x_train, x_test, y_train, y_test = get_data(4)\n",
    "\n",
    "# # x_train = torch.FloatTensor(sc.fit_transform(x_train))\n",
    "# # x_test = torch.FloatTensor(sc.transform(x_test))\n",
    "# # y_train = torch.FloatTensor(sc.fit_transform(y_train))\n",
    "\n",
    "# initial_x = torch.FloatTensor(np.round(x_train[:x_train.shape[1]+1],0))\n",
    "# initial_y = torch.FloatTensor(np.round(y_train[:x_train.shape[1]+1],0))\n",
    "\n",
    "# x_train = torch.FloatTensor(x_train[x_train.shape[1]+1:])\n",
    "# y_train = torch.FloatTensor(y_train[x_train.shape[1]+1:])\n",
    "\n",
    "# network = Network(4,initial_x,initial_y)\n",
    "# initializing(network, initial_x, initial_y)\n",
    "# print(\"<<Initializing後看一下差異>>\")\n",
    "# yo,loss = network.forward()\n",
    "# print(torch.abs(network.y-yo))\n",
    "\n",
    "# for i in range(0, 1):\n",
    "    \n",
    "#     print(\"現在訓練到第幾筆資料: %d\"%(i+x_train.shape[1]+2))\n",
    "    \n",
    "#     sorted_index = selecting(network, x_train, y_train)\n",
    "#     ## Add new data for training\n",
    "#     network.addData(x_train[sorted_index[0]], y_train[sorted_index[0]])\n",
    "#     print(\"現在要進去模型的數據，索引%d，y=\"%(sorted_index[0]),y_train[sorted_index[0]].data)\n",
    "#     x_train = np.delete(x_train, sorted_index[0], 0)\n",
    "#     y_train = np.delete(y_train, sorted_index[0], 0)\n",
    "    \n",
    "#     print(\"<<(前)差異>>\")\n",
    "#     yo,loss = network.forward()\n",
    "#     print(torch.abs(network.y-yo))\n",
    "    \n",
    "#     network = matching_for_reorganizing(network)\n",
    "    \n",
    "#     print(\"<<(後)差異>>\")\n",
    "#     yo,loss = network.forward()\n",
    "#     print(torch.abs(network.y-yo))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cramming module (Check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cramming(network):\n",
    "    \n",
    "    torch.random.manual_seed(0)\n",
    "    print(\"<<Cramming module>>\")\n",
    "\n",
    "    \n",
    "    ## Find unsatisfied data:K\n",
    "    yo, loss = network.forward()\n",
    "    undesired_index = torch.nonzero(torch.abs(yo-network.y) > network.threshold_for_error)\n",
    "\n",
    "    ## Print out the undesired_index for debug\n",
    "    print(\"The index of the undesired data:\",undesired_index)\n",
    "\n",
    "\n",
    "    # Unsatisfied situation\n",
    "    ## Find the index of the unsatisfied data\n",
    "    k_data_num = undesired_index[0][0]\n",
    "\n",
    "    undesired_data = torch.reshape(network.x[k_data_num,:], [1,-1])\n",
    "\n",
    "    ## Remove the data that does not meet the error term\n",
    "    left_data = network.x[:k_data_num,:]\n",
    "    right_data = network.x[k_data_num+1:,:]\n",
    "    remain_tensor = torch.cat([left_data, right_data], 0)\n",
    "\n",
    "        \n",
    "    ## Use the random method to find out the gamma and zeta\n",
    "    while True:\n",
    "\n",
    "        ## Find m-vector gamma: r\n",
    "        ## Use the random method to generate the gamma that can make the conditions met\n",
    "        gamma = torch.rand(size=[1,network.x.shape[1]]).cuda()\n",
    "        subtract_undesired_data = torch.sub(remain_tensor, undesired_data)\n",
    "        matmul_value = torch.mm(gamma,torch.t(subtract_undesired_data))\n",
    "\n",
    "        if torch.all(matmul_value != 0):\n",
    "            break\n",
    "\n",
    "    while True:\n",
    "\n",
    "        ## Find the tiny value: zeta\n",
    "        ## Use the random method to generate the zeta that can make the conditions met\n",
    "        zeta = torch.rand(size=[1]).cuda()\n",
    "        \n",
    "        if torch.all(torch.mul(torch.add(zeta,matmul_value),torch.sub(zeta,matmul_value))<0):\n",
    "            break\n",
    "\n",
    "    for i in range(undesired_index.shape[0]):\n",
    "        \n",
    "        k_l = undesired_index[i][1]\n",
    "#         print(\"The output node:\",k_l)\n",
    "        ## The weight of input layer to hidden layer I\n",
    "        w10 = gamma\n",
    "        w11 = gamma\n",
    "        w12 = gamma\n",
    "        \n",
    "        W1_new = torch.cat([w10,w11,w12],0)\n",
    "#         print(\"W1_new.shape:\",W1_new.shape)\n",
    "        \n",
    "        ## The bias of input layer to hidden layer I\n",
    "        matual_value = torch.mm(gamma,torch.t(undesired_data))\n",
    "\n",
    "        b10 = torch.sub(zeta,matual_value)\n",
    "        b11 = -1*matual_value\n",
    "        b12 = torch.sub(-1*zeta,matual_value)\n",
    "        \n",
    "        \n",
    "        b1_new = torch.reshape(torch.cat([b10,b11,b12],0),[3])\n",
    "       \n",
    "#         print(\"b1_new\",b1_new)\n",
    "    \n",
    "    \n",
    "        ## The weight of hidden layer I to output layer\n",
    "        gap = network.y[k_data_num, k_l]-yo[k_data_num, k_l]\n",
    "#         print(\"gap:\",gap)\n",
    "        \n",
    "        wo0_value = gap/zeta\n",
    "        wo1_value = (-2*gap)/zeta\n",
    "        wo2_value = gap/zeta\n",
    "\n",
    "        index = torch.tensor([[k_l]])\n",
    "    \n",
    "        wo0 = torch.FloatTensor(torch.zeros(1, 4).scatter_(1, index, 1)).cuda() * wo0_value\n",
    "        wo1 = torch.FloatTensor(torch.zeros(1, 4).scatter_(1, index, 1)).cuda() * wo1_value\n",
    "        wo2 = torch.FloatTensor(torch.zeros(1, 4).scatter_(1, index, 1)).cuda() * wo2_value\n",
    "        \n",
    "        \n",
    "#         print(\"Wo0\",wo0_value)\n",
    "#         print(\"Wo1\",wo1_value)\n",
    "#         print(\"Wo2\",wo2_value)\n",
    "            \n",
    "        Wo_new = torch.t(torch.cat([wo0,wo1,wo2],0))\n",
    "        \n",
    "#         print(\"Wo_new.shape\",Wo_new.shape)\n",
    "        \n",
    "        ## Add new neuroes to the network\n",
    "        network.linear1.weight = torch.nn.Parameter(torch.cat([network.linear1.weight.data, W1_new]))\n",
    "        network.linear1.bias = torch.nn.Parameter(torch.cat([network.linear1.bias.data, b1_new]))\n",
    "        network.linear2.weight = torch.nn.Parameter(torch.cat([network.linear2.weight.data, Wo_new],1))\n",
    "#         print(network.state_dict())\n",
    "#         yo, loss = network.forward()\n",
    "\n",
    "#         print(torch.abs(network.y-yo))\n",
    "    \n",
    "    yo, loss = network.forward()\n",
    "    ## Determine if cramming is successful and print out the corresponding information\n",
    "    if torch.all(torch.abs(yo[k_data_num,k_l]-network.y[k_data_num,k_l]) <= network.threshold_for_error):\n",
    "        network.acceptable = True \n",
    "        print(\"Cramming success!\")\n",
    "        \n",
    "    else:\n",
    "        print(\"Cramming failed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train_data, x_test_data, y_train_data, y_test_data = get_data(4)\n",
    "\n",
    "# x_train = torch.FloatTensor(x_train_data)\n",
    "# x_test = torch.FloatTensor(x_test_data)\n",
    "# y_train = torch.FloatTensor(y_train_data)\n",
    "\n",
    "\n",
    "# initial_x = x_train_scaled[:x_train_scaled.shape[1]+1]\n",
    "# initial_y = y_train_scaled[:x_train_scaled.shape[1]+1]\n",
    "\n",
    "# x_train = x_train_scaled[x_train_scaled.shape[1]+1:]\n",
    "# y_train = y_train_scaled[x_train_scaled.shape[1]+1:]\n",
    "\n",
    "# # print(initial_x)\n",
    "# # print(initial_y)\n",
    "\n",
    "# network = Network(4,initial_x,initial_y)\n",
    "# initializing(network, initial_x, initial_y)\n",
    "# sorted_index = selecting(network, x_train, y_train)\n",
    "# network.addData(x_train[sorted_index[0]], y_train[sorted_index[0]])\n",
    "\n",
    "# yo,loss = network.forward()\n",
    "# print(\"<<誤差>>\")\n",
    "# print(torch.abs(yo-network.y))\n",
    "\n",
    "# reorganizing(network)\n",
    "\n",
    "# print(type(network.parameters()))\n",
    "# print(list(network.parameters())[0].device)\n",
    "\n",
    "# print(network.x.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularizing module (Check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regularizing(network):\n",
    "\n",
    "    print(\"<<Regularizing module>>\")\n",
    "    ## Record the number of executions\n",
    "    times_enlarge = 0\n",
    "    times_shrink = 0\n",
    "    ## Set up the learning rate of the network\n",
    "    network.learning_rate = 1e-3\n",
    "\n",
    "    ## Set epoch to 100\n",
    "    for i in range(100):\n",
    "\n",
    "        ## Store the parameter of the network\n",
    "        network_pre = copy.deepcopy(network)\n",
    "        yo, loss = network.forward(1e-3)\n",
    "        loss_pre = loss\n",
    "\n",
    "#         print(\"調整前的network\")\n",
    "#         print(\"<<變數>>\")\n",
    "#         print(network.state_dict())\n",
    "#         print(\"<<Loss值>>\")\n",
    "#         print(loss)\n",
    "#         print(\"差異\")\n",
    "#         print(torch.abs(yo-network.y))\n",
    "        \n",
    "        ## Backward operation to obtain w'\n",
    "        network.backward_Adam(loss)\n",
    "        yo, loss = network.forward(1e-3)\n",
    "#         print(\"調整後的network\")\n",
    "#         print(\"<<變數>>\")\n",
    "#         print(network.state_dict())\n",
    "#         print(\"<<Loss值>>\")\n",
    "#         print(loss)\n",
    "#         print(\"差異\")\n",
    "#         print(torch.abs(yo-network.y))\n",
    "         # Confirm whether the adjusted loss value is smaller than the current one\n",
    "        if loss <= loss_pre:\n",
    "            \n",
    "            ## Identify that all forecast value has met the error term\n",
    "            if torch.all(torch.abs(yo-network.y) <= network.threshold_for_error):\n",
    "                \n",
    "                ## If true, multiply the learning rate by 1.2\n",
    "                network.learning_rate *= 1.2\n",
    "                times_enlarge += 1\n",
    "#                 print(\"Regularizing %d process - Enlarge\"%i)\n",
    "                print(\"第\\\"%d\\\"回合是成功執行regularizing\"%(i+1))\n",
    "                print(\"差異\")\n",
    "                print(torch.abs(yo-network.y))\n",
    "\n",
    "            else:\n",
    "\n",
    "                ## Else, restore w and end the process\n",
    "                network = network_pre\n",
    "                print(\"Number of enlarge:\",times_enlarge)\n",
    "                print(\"Number of shrink:\",times_shrink)\n",
    "#                 print(\"Regularizing result: Unable to meet the error term\")\n",
    "                print(\"結束Regularizing，因為沒有顧好預測誤差\")\n",
    "                return(network)\n",
    "\n",
    "        # If the adjusted loss value is not smaller than the current one\n",
    "        else:\n",
    "\n",
    "            ## If the learning rate is greater than the threshold for learning rate\n",
    "            if network.learning_rate > network.threshold_for_lr:\n",
    "                \n",
    "                ## Restore the w and multiply the learning rate by 0.7\n",
    "                network = network_pre\n",
    "                network.learning_rate *= 0.7\n",
    "                times_shrink += 1\n",
    "                print(\"我把Learning rate變小\")\n",
    "#                 print(\"Regularizing %d process - Shrink\"%i)\n",
    "             ## If the learning rate is smaller than the threshold for learning rate\n",
    "            else:\n",
    "                \n",
    "                ## Restore the w\n",
    "                network = network_pre\n",
    "                print(\"Number of enlarge:\",times_enlarge)\n",
    "                print(\"Number of shrink:\",times_shrink)\n",
    "#                 print(\"Regularizing result: Less than the epsilon for the learning rate\")\n",
    "                print(\"結束Regularizing，因為Learning不能這麼小啦\")\n",
    "                return(network)\n",
    "\n",
    "    print(\"第\\\"%d\\\"回合Regularizing module完畢\"%(i+1))\n",
    "    print(\"Number of enlarge:\",times_enlarge)\n",
    "    print(\"Number of shrink:\",times_shrink)\n",
    "#             print(\"Regularizing result: The number of rounds has reached\")\n",
    "    return(network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reorganizing module (Check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reorganizing(network):\n",
    "    print(\"<<Reorganizing module>>\")\n",
    "    ## Set up the k = 1, and p = the number of hidden node\n",
    "    k = 1\n",
    "#     p = network.W1.shape[1]\n",
    "    p = network.linear1.weight.data.shape[0]\n",
    "    \n",
    "    while True:\n",
    "\n",
    "        ## If k > p, end of Process\n",
    "        if k > p:\n",
    "\n",
    "            print(\"Reorganizing result: The final number of neuro is \",p)\n",
    "            return(network)\n",
    "\n",
    "        ## Else, Process is ongoing\n",
    "        else:\n",
    "\n",
    "            ## Using the regularizing module to adjust the network\n",
    "            network = regularizing(network)\n",
    "            \n",
    "            ## Store the network and w\n",
    "            network_pre = copy.deepcopy(network)\n",
    "\n",
    "            ## Set up the acceptable of the network as false\n",
    "            network.acceptable = False\n",
    "\n",
    "            ## Ignore the K hidden node\n",
    "            network.linear1.weight = torch.nn.Parameter(torch.cat([network.linear1.weight[:k-1],network.linear1.weight[k:]],0))\n",
    "            network.linear1.bias = torch.nn.Parameter(torch.cat([network.linear1.bias[:k-1],network.linear1.bias[k:]]))\n",
    "            network.linear2.weight = torch.nn.Parameter(torch.cat([network.linear2.weight[:,:k-1],network.linear2.weight[:,k:]],1))\n",
    "\n",
    "            ## Using the matching module to adjust the network\n",
    "            network = matching(network)\n",
    "            \n",
    "            print(\"是不是可以不要你:\",network.acceptable)\n",
    "            \n",
    "            ## If the resulting network is acceptable, this means that the k hidden node can be removed\n",
    "            if network.acceptable and p!=1:\n",
    "\n",
    "                print(\"Drop out the nero number: %d / %d\" %(k, p))\n",
    "                network.nb_node_pruned += 1\n",
    "                ## p--\n",
    "                p-=1\n",
    "\n",
    "            ## Else, it means that the k hidden node cannot be removed\n",
    "            else:\n",
    "                \n",
    "                ## Restore the network and w\n",
    "                network = network_pre\n",
    "                print(\"Cannot drop out the nero number: %d / %d\" %(k, p))\n",
    "                \n",
    "                ## k++\n",
    "                k+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, nb_neuro, x_train_scaled, y_train_scaled):\n",
    "        \n",
    "        super(Network, self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(x_train_scaled.shape[1], nb_neuro).cuda()\n",
    "        self.linear2 = torch.nn.Linear(nb_neuro, y_train_scaled.shape[1]).cuda()\n",
    "        \n",
    "        \n",
    "        # Stop criteria - threshold\n",
    "        self.threshold_for_error = 2000\n",
    "        self.threshold_for_lr = 1e-4\n",
    "        \n",
    "        # Input data \n",
    "        self.x = torch.FloatTensor(x_train_scaled).cuda()\n",
    "        self.y = torch.FloatTensor(y_train_scaled).cuda()\n",
    "        \n",
    "        # Learning rate\n",
    "        self.learning_rate = 1e-2\n",
    "        \n",
    "        # Whether the network is acceptable, default as False\n",
    "        self.acceptable = False\n",
    "        \n",
    "        # Some record for experiment\n",
    "        self.nb_node_pruned = 0\n",
    "        self.nb_node_acceptable=torch.IntTensor([nb_neuro])\n",
    "        \n",
    "    ## Forecast the test data\n",
    "    def forecast(self, x_test):\n",
    "    \n",
    "        x_test = torch.FloatTensor(x_test)\n",
    "        activation_value = self.linear1(x_test).clamp(min=0)\n",
    "        forecast_value = self.linear2(activation_value)\n",
    "       \n",
    "        return forecast_value\n",
    "\n",
    "    ## Reset the x and y data\n",
    "    def setData(self, x_train_scaled, y_train_scaled):\n",
    "        self.x = torch.FloatTensor(x_train_scaled).cuda()\n",
    "        self.y = torch.FloatTensor(y_train_scaled).cuda()\n",
    "    \n",
    "    ## Add the new data to the x and y data\n",
    "    def addData(self, new_x_train, new_y_train):\n",
    "\n",
    "        self.x = torch.cat([self.x, new_x_train.reshape(1,-1).cuda()],0)\n",
    "        self.y = torch.cat([self.y, new_y_train.reshape(1,-1).cuda()],0)\n",
    "    \n",
    "    ## forward operation\n",
    "    def forward(self, reg_strength=0):\n",
    "       \n",
    "        y1 = self.linear1(self.x).clamp(min=0)\n",
    "        yo = self.linear2(y1)\n",
    "\n",
    "        # performance measure\n",
    "        param_val= torch.sum(torch.pow(self.linear2.bias.data,2))+torch.sum(torch.pow(self.linear2.weight.data,2))+torch.sum(torch.pow(self.linear1.bias.data,2))+torch.sum(torch.pow(self.linear1.weight.data,2))\n",
    "        reg_term= reg_strength/((self.linear2.bias.data.shape[0]*(self.linear2.weight.data.shape[1]+1)) +(self.linear1.bias.data.shape[0]*(self.linear1.weight.data.shape[1]+1)))*param_val\n",
    "        loss = torch.nn.functional.mse_loss(yo,self.y)+reg_term\n",
    "        loss = loss.cuda()\n",
    "        return(yo, loss)\n",
    "\n",
    "    # backward operation\n",
    "    def backward_Adam(self,loss):    \n",
    "\n",
    "        optimizer = optim.Adadelta(self.parameters(), lr=self.learning_rate)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main function. Construct a instance of network\n",
    "- trained through the matching module, reorganizing module, and cramming module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing module\n",
      "<<Initializing後看一下差異>>\n",
      "tensor([[ 38.7969, 161.9844,  31.5156,  15.4375],\n",
      "        [149.3125, 225.2031,  91.0469,  28.4414],\n",
      "        [271.9062, 147.3750, 326.8047, 299.5664],\n",
      "        [  0.0000, 199.0938,  43.7500,  79.3359],\n",
      "        [255.9375, 305.9688,  85.8359, 127.3828],\n",
      "        [388.1875, 174.7656,   1.3125, 127.6992],\n",
      "        [375.7344, 373.0938, 238.7188, 197.6367],\n",
      "        [351.1719, 298.5938, 260.4766, 121.7500],\n",
      "        [635.8438,  77.9531,  19.9219, 388.8672],\n",
      "        [ 46.7031, 406.1719, 562.6875, 459.5234],\n",
      "        [182.8594,  53.5312, 443.7969,   0.8633],\n",
      "        [585.0469, 235.4531,  38.3125,  10.6523],\n",
      "        [237.5625, 114.3281,  86.8672, 296.6055],\n",
      "        [131.2969, 259.0000, 597.3672, 320.1484],\n",
      "        [252.5312, 400.0938, 760.3984, 207.0039],\n",
      "        [262.9844, 147.7656,  96.6797, 144.7930],\n",
      "        [204.4844, 611.0156, 375.1016, 593.8320],\n",
      "        [193.6875,  63.5625, 288.8672, 241.9414],\n",
      "        [ 51.6250, 208.4375, 465.3672,  22.1250]], device='cuda:0',\n",
      "       grad_fn=<AbsBackward>)\n",
      "現在訓練到第幾筆資料: 20\n",
      "<<Selecting module>>\n",
      "The loss value of k: (2718024.5, 0)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引0，y= tensor([58530., 57130., 57772., 58348.])\n",
      "<<差異>>\n",
      "tensor([[   38.7969,   161.9844,    31.5156,    15.4375],\n",
      "        [  149.3125,   225.2031,    91.0469,    28.4414],\n",
      "        [  271.9062,   147.3750,   326.8047,   299.5664],\n",
      "        [    0.0000,   199.0938,    43.7500,    79.3359],\n",
      "        [  255.9375,   305.9688,    85.8359,   127.3828],\n",
      "        [  388.1875,   174.7656,     1.3125,   127.6992],\n",
      "        [  375.7344,   373.0938,   238.7188,   197.6367],\n",
      "        [  351.1719,   298.5938,   260.4766,   121.7500],\n",
      "        [  635.8438,    77.9531,    19.9219,   388.8672],\n",
      "        [   46.7031,   406.1719,   562.6875,   459.5234],\n",
      "        [  182.8594,    53.5312,   443.7969,     0.8633],\n",
      "        [  585.0469,   235.4531,    38.3125,    10.6523],\n",
      "        [  237.5625,   114.3281,    86.8672,   296.6055],\n",
      "        [  131.2969,   259.0000,   597.3672,   320.1484],\n",
      "        [  252.5312,   400.0938,   760.3984,   207.0039],\n",
      "        [  262.9844,   147.7656,    96.6797,   144.7930],\n",
      "        [  204.4844,   611.0156,   375.1016,   593.8320],\n",
      "        [  193.6875,    63.5625,   288.8672,   241.9414],\n",
      "        [   51.6250,   208.4375,   465.3672,    22.1250],\n",
      "        [  375.7031,  2286.5781,  1752.3906,  1559.3906]], device='cuda:0',\n",
      "       grad_fn=<AbsBackward>)\n",
      "<<Matching module>>\n",
      "Matching finished - the network is acceptable\n",
      "Number of enlarge: 102\n",
      "Number of shrink: 45\n",
      "<<Matching後看一下差異>>\n",
      "tensor([[   71.8438,   138.5547,    20.8555,    14.7734],\n",
      "        [  181.4492,   242.2812,    97.9570,    29.8359],\n",
      "        [  239.7930,   149.1953,   322.6953,   288.8398],\n",
      "        [    0.1562,   200.6484,    47.4531,    68.3438],\n",
      "        [  224.2227,   326.2734,   107.7539,   153.6523],\n",
      "        [  420.0820,   138.5664,    35.0469,   168.5234],\n",
      "        [  344.2852,   420.7656,   284.8359,   246.2383],\n",
      "        [  320.3164,   215.3789,   185.6250,    51.6992],\n",
      "        [  666.7461,    12.6719,    60.0078,   315.7188],\n",
      "        [   15.7812,   504.1016,   646.4805,   532.6172],\n",
      "        [  213.8047,    75.7773,   336.1602,    90.6094],\n",
      "        [  554.2617,    78.6094,    88.3828,   114.6094],\n",
      "        [  268.1875,   294.0547,   229.2578,   411.2227],\n",
      "        [  161.3906,   455.9141,   752.1719,   197.2266],\n",
      "        [  222.7656,   174.2266,   584.2969,    68.3984],\n",
      "        [  292.2461,    73.8281,   268.4375,    11.1797],\n",
      "        [  175.6211,   833.6914,   545.1250,   723.1680],\n",
      "        [  222.2227,   169.9727,   465.4883,   109.3359],\n",
      "        [   23.7617,    41.8828,   276.9766,   118.0117],\n",
      "        [  347.9141,  1998.1094,  1533.1523,  1392.7773]], device='cuda:0',\n",
      "       grad_fn=<AbsBackward>)\n",
      "<<Reorganizing module>>\n",
      "<<Regularizing module>>\n",
      "第\"1\"回合是成功執行regularizing\n",
      "差異\n",
      "tensor([[   69.6523,   136.3945,    18.7070,    16.9336],\n",
      "        [  179.2930,   244.4062,   100.0508,    27.7266],\n",
      "        [  241.8945,   151.2734,   324.7500,   290.9062],\n",
      "        [    0.1484,   202.6875,    45.4258,    70.3867],\n",
      "        [  226.2539,   324.2578,   105.7539,   151.6445],\n",
      "        [  418.0195,   140.5703,    33.0352,   166.5273],\n",
      "        [  346.3438,   418.7656,   282.8398,   244.2656],\n",
      "        [  322.3555,   217.3594,   187.5859,    53.6484],\n",
      "        [  664.6875,    10.6719,    58.0352,   317.6797],\n",
      "        [   17.8203,   502.0859,   644.4844,   530.6367],\n",
      "        [  211.7656,    73.7930,   338.1484,    88.6445],\n",
      "        [  556.2734,    80.5977,    86.4219,   112.6602],\n",
      "        [  266.1953,   292.0859,   227.3203,   409.3125],\n",
      "        [  159.4258,   454.0078,   750.2656,   199.1094],\n",
      "        [  224.6875,   176.0898,   586.1719,    70.2578],\n",
      "        [  290.3477,    71.9648,   266.5859,    13.0078],\n",
      "        [  177.4844,   831.8438,   543.2852,   721.3672],\n",
      "        [  220.3984,   168.1719,   463.6875,   111.1172],\n",
      "        [   25.5508,    40.1328,   278.7188,   116.2734],\n",
      "        [  349.6875,  1999.7969,  1534.8750,  1394.4688]], device='cuda:0',\n",
      "       grad_fn=<AbsBackward>)\n",
      "Number of enlarge: 1\n",
      "Number of shrink: 0\n",
      "結束Regularizing，因為沒有顧好預測誤差\n",
      "<<Matching module>>\n",
      "Matching finished - the network is acceptable\n",
      "Number of enlarge: 513\n",
      "Number of shrink: 252\n",
      "是不是可以不要你: True\n",
      "Drop out the nero number: 1 / 4\n",
      "<<Regularizing module>>\n",
      "第\"1\"回合是成功執行regularizing\n",
      "差異\n",
      "tensor([[ 735.1914,  785.3203,  843.9141,  855.0742],\n",
      "        [1959.6562, 1367.6406, 1164.3477, 1022.3359],\n",
      "        [ 266.7266, 1191.3750, 1319.3047, 1265.8164],\n",
      "        [1351.3633, 1181.3750,  890.4844,  982.3555],\n",
      "        [1477.1562,  393.3750,  587.0703,  501.9844],\n",
      "        [1473.4336,  447.5039,  265.0430,   73.7109],\n",
      "        [1827.4727,  345.9883,  213.4023,  247.8242],\n",
      "        [ 234.5000,   44.6055,   63.3242,  292.3906],\n",
      "        [1179.0625,  226.1562,  261.2305,   17.1836],\n",
      "        [1320.7266,  509.8477,  648.5391,  622.9102],\n",
      "        [1651.4922,  259.0547,  170.4141,  358.2031],\n",
      "        [1115.5547,   78.7734,  224.6797,  356.7109],\n",
      "        [1137.8242,  445.0938,  355.8984,  641.8242],\n",
      "        [1645.0117,  600.1602,  867.0156,   23.2773],\n",
      "        [ 977.0000,  154.3789,  293.5391,  341.5156],\n",
      "        [1795.2148,  232.8516,  392.5000,  226.6094],\n",
      "        [1938.6562,  708.5312,  395.7617,  673.4102],\n",
      "        [1998.8398,   20.0273,  251.7891,  223.7734],\n",
      "        [1820.6172,  175.3828,  524.6836,   28.6094],\n",
      "        [ 411.7109, 1716.8828, 1310.2930, 1039.5352]], device='cuda:0',\n",
      "       grad_fn=<AbsBackward>)\n",
      "第\"2\"回合是成功執行regularizing\n",
      "差異\n",
      "tensor([[ 735.5859,  782.9961,  841.4297,  857.8945],\n",
      "        [1960.0469, 1365.3867, 1161.9297, 1025.0938],\n",
      "        [ 266.3398, 1189.1328, 1316.9180, 1268.5352],\n",
      "        [1350.9883, 1179.1719,  888.1445,  985.0430],\n",
      "        [1476.7852,  391.1680,  584.7266,  504.6523],\n",
      "        [1473.0859,  445.3086,  262.6914,   76.3867],\n",
      "        [1827.1406,  348.1641,  215.7344,  245.1719],\n",
      "        [ 234.1836,   46.7617,   65.6289,  289.7773],\n",
      "        [1179.3828,  228.2969,  263.5508,   19.8047],\n",
      "        [1321.0625,  512.0234,  650.8672,  620.2812],\n",
      "        [1651.8164,  261.2266,  168.1133,  355.5820],\n",
      "        [1115.8711,   80.9180,  226.9570,  354.1094],\n",
      "        [1138.1328,  447.1758,  358.1562,  639.2617],\n",
      "        [1645.3203,  602.2266,  869.2500,   20.7539],\n",
      "        [ 977.2891,  156.3906,  291.3398,  339.0352],\n",
      "        [1795.5078,  234.8438,  394.6523,  224.1680],\n",
      "        [1938.9609,  710.5000,  397.8672,  671.0117],\n",
      "        [1999.1406,   18.0938,  253.8594,  226.1289],\n",
      "        [1820.9062,  173.5078,  522.6719,   30.9023],\n",
      "        [ 411.9805, 1715.0195, 1308.2969, 1041.8008]], device='cuda:0',\n",
      "       grad_fn=<AbsBackward>)\n",
      "第\"3\"回合是成功執行regularizing\n",
      "差異\n",
      "tensor([[ 736.0547,  780.2109,  838.4766,  861.2773],\n",
      "        [1960.5273, 1362.6289, 1159.0352, 1028.4062],\n",
      "        [ 265.8672, 1186.4336, 1314.0586, 1271.7930],\n",
      "        [1350.5273, 1176.4961,  885.3359,  988.2695],\n",
      "        [1476.3398,  388.5312,  581.9141,  507.8477],\n",
      "        [1472.6602,  442.6484,  259.8828,   79.5938],\n",
      "        [1826.7383,  350.7734,  218.5117,  242.0078],\n",
      "        [ 233.8047,   49.3438,   68.3867,  286.6328],\n",
      "        [1179.7617,  230.8945,  266.2891,   22.9492],\n",
      "        [1321.4531,  514.6250,  653.6250,  617.1250],\n",
      "        [1652.1992,  263.8203,  165.3359,  352.4297],\n",
      "        [1116.2422,   83.4531,  229.7031,  350.9922],\n",
      "        [1138.5117,  449.7070,  360.8672,  636.1875],\n",
      "        [1645.6836,  604.6992,  871.8984,   17.7305],\n",
      "        [ 977.6367,  158.8320,  288.7422,  336.0742],\n",
      "        [1795.8633,  237.2266,  397.2266,  221.2578],\n",
      "        [1939.3164,  712.8281,  400.4023,  668.1328],\n",
      "        [1999.4961,   15.7812,  256.3477,  228.9570],\n",
      "        [1821.2539,  171.2617,  520.2617,   33.6406],\n",
      "        [ 412.2930, 1712.8242, 1305.9062, 1044.5156]], device='cuda:0',\n",
      "       grad_fn=<AbsBackward>)\n",
      "第\"4\"回合是成功執行regularizing\n",
      "差異\n",
      "tensor([[ 736.5898,  776.8164,  835.1094,  865.3320],\n",
      "        [1961.0586, 1359.3359, 1155.7578, 1032.3594],\n",
      "        [ 265.3438, 1183.1836, 1310.8086, 1275.6953],\n",
      "        [1350.0117, 1173.2891,  882.1172,  992.1172],\n",
      "        [1475.8438,  385.3398,  578.7422,  511.6641],\n",
      "        [1472.1953,  439.4766,  256.6914,   83.4258],\n",
      "        [1826.2852,  353.9570,  221.6719,  238.2070],\n",
      "        [ 233.3828,   52.4727,   71.5078,  282.8828],\n",
      "        [1180.1914,  234.0664,  269.4219,   26.7148],\n",
      "        [1321.8906,  517.7734,  656.7617,  613.3555],\n",
      "        [1652.6172,  266.9336,  162.2227,  348.6797],\n",
      "        [1116.6602,   86.5508,  232.8086,  347.2617],\n",
      "        [1138.9141,  452.7344,  363.9023,  632.5234],\n",
      "        [1646.0859,  607.6953,  874.8789,   14.1289],\n",
      "        [ 978.0156,  161.7617,  285.8086,  332.5312],\n",
      "        [1796.2422,  240.1172,  400.1055,  217.7656],\n",
      "        [1939.7188,  715.6992,  403.2695,  664.6836],\n",
      "        [1999.8867,   12.9844,  259.1367,  232.3438],\n",
      "        [1821.6406,  168.5156,  517.5430,   36.9414],\n",
      "        [ 412.6406, 1710.1211, 1303.2344, 1047.7656]], device='cuda:0',\n",
      "       grad_fn=<AbsBackward>)\n",
      "Number of enlarge: 4\n",
      "Number of shrink: 0\n",
      "結束Regularizing，因為沒有顧好預測誤差\n",
      "<<Matching module>>\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-3752e8e4f14e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0mnb_step4\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m     \u001b[0mnetwork\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreorganizing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m     \u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnb_node_acceptable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnb_node_acceptable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIntTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnb_node_acceptable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-27-c3e5205e4a47>\u001b[0m in \u001b[0;36mreorganizing\u001b[0;34m(network)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0;31m## Using the matching module to adjust the network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m             \u001b[0mnetwork\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmatching\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"是不是可以不要你:\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macceptable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-21-dadfbd5b2c44>\u001b[0m in \u001b[0;36mmatching\u001b[0;34m(network)\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0;31m# Backward and check the loss performance of the network with new learning rate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward_Adam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0myo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;31m#         print(\"調整後看一下\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-28-f4dd1627898b>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, reg_strength)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;31m# performance measure\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0mparam_val\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m         \u001b[0mreg_term\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mreg_strength\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mparam_val\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myo\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mreg_term\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## Record program start time\n",
    "start = time.time()\n",
    "\n",
    "## Record the number of each step\n",
    "nb_step4 = 0\n",
    "nb_step6_1 = 0\n",
    "nb_step6_2 = 0\n",
    "\n",
    "x_train, x_test, y_train, y_test = get_data(4)\n",
    "\n",
    "# x_train = torch.FloatTensor(sc.fit_transform(x_train))\n",
    "# x_test = torch.FloatTensor(sc.transform(x_test))\n",
    "# y_train = torch.FloatTensor(sc.fit_transform(y_train))\n",
    "\n",
    "initial_x = torch.FloatTensor(np.round(x_train[:x_train.shape[1]+1],0))\n",
    "initial_y = torch.FloatTensor(np.round(y_train[:x_train.shape[1]+1],0))\n",
    "\n",
    "x_train = torch.FloatTensor(x_train[x_train.shape[1]+1:])\n",
    "y_train = torch.FloatTensor(y_train[x_train.shape[1]+1:])\n",
    "\n",
    "network = Network(4,initial_x,initial_y)\n",
    "initializing(network, initial_x, initial_y)\n",
    "print(\"<<Initializing後看一下差異>>\")\n",
    "yo,loss = network.forward()\n",
    "print(torch.abs(network.y-yo))\n",
    "\n",
    "for i in range(0, 30):\n",
    "    \n",
    "    print(\"現在訓練到第幾筆資料: %d\"%(i+x_train.shape[1]+2))\n",
    "    \n",
    "    sorted_index = selecting(network, x_train, y_train)\n",
    "    ## Add new data for training\n",
    "    network.addData(x_train[sorted_index[0]], y_train[sorted_index[0]])\n",
    "    print(\"現在要進去模型的數據，索引%d，y=\"%(sorted_index[0]),y_train[sorted_index[0]].data)\n",
    "    x_train = np.delete(x_train, sorted_index[0], 0)\n",
    "    y_train = np.delete(y_train, sorted_index[0], 0)\n",
    "    \n",
    "    print(\"<<差異>>\")\n",
    "    yo,loss = network.forward()\n",
    "    print(torch.abs(network.y-yo))\n",
    "    \n",
    "#     print(\"<<Before>>\")\n",
    "#     yo,loss = network.forward()\n",
    "#     print(network.learning_rate)\n",
    "#     print(network.linear1.weight)\n",
    "#     print(torch.abs(yo-network.y))\n",
    "#     print(network.state_dict())\n",
    "\n",
    "    if not torch.all(torch.abs(network.y-yo)<network.threshold_for_error):\n",
    "        \n",
    "        network.acceptable = False\n",
    "        network = matching(network)\n",
    "    \n",
    "        print(\"<<Matching後看一下差異>>\")\n",
    "        yo,loss = network.forward()\n",
    "        print(torch.abs(network.y-yo))\n",
    "    \n",
    "        if network.acceptable == False:\n",
    "            cramming(network)\n",
    "\n",
    "            print(\"<<Cramming後看一下差異>>\")\n",
    "            yo,loss = network.forward()\n",
    "            print(torch.abs(network.y-yo))\n",
    "            nb_step6_2 += 1\n",
    "        \n",
    "        else:\n",
    "            nb_step6_1 += 1\n",
    "        \n",
    "    else:\n",
    "        nb_step4 += 1\n",
    "    \n",
    "    network = reorganizing(network)\n",
    "    network.nb_node_acceptable = torch.cat([network.nb_node_acceptable, torch.IntTensor([network.linear1.bias.data.shape[0]])],0)\n",
    "    print(network.nb_node_acceptable)\n",
    "    print(\"<<Reorganizing後看一下差異>>\")\n",
    "    yo,loss = network.forward()\n",
    "    print(torch.abs(network.y-yo))\n",
    "    print(\"-\"*90)\n",
    "    #     yo,loss = network.forward()\n",
    "#     print(torch.abs(network.y-yo))\n",
    "print(list(network.parameters())[0].device)\n",
    "end = time.time()\n",
    "#     matching(network)\n",
    "# print(\"<<Main>>\")\n",
    "# print(network.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation(network, nb_step4, nb_step6_1, nb_step6_2, x_test, y_test, start, end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def accuracy_cacl(pred_value, actual_value):\n",
    "    \n",
    "# #     yo, loss, tape = network.forward()\n",
    "#     accuracy = []\n",
    "\n",
    "#     for i in range(pred_value.shape[1]):\n",
    "        \n",
    "#         correct_times = torch.nonzero(torch.abs(pred_value[:,i].data - actual_value[:,i].data) < 2000)\n",
    "#         accuracy.append(correct_times.shape[0]/pred_value.shape[0])   \n",
    "        \n",
    "#     return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # yo,loss = network.forward()\n",
    "# accuracy_cacl(yo, network.y)\n",
    "\n",
    "# torch.nonzero(yo[:,1].data-network.y[:,1].data< 2000).shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_result(\"Training\", yo, network.y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
